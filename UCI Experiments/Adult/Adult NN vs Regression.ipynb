{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To identify the appropriate datasets, a subset of candidate datasets with appropriate characteristics will be chosen. The draft characteristics are:\n",
    "* Over 10,000 data points\n",
    "* Binary classification problem\n",
    "* Over 10 data fields in the dataset\n",
    "\n",
    "For each dataset, the following protocol will be applied:\n",
    "* Divide the dataset into 10 equal, randomly allocated folds\n",
    "* For each fold:\n",
    "\t* Train the model with the fold being the test set\n",
    "\t\t* N.B. optionally, with the other 9 folds, one can be explicitly held out as a validation step for hyper-parameter management\n",
    "\t* Record the AUC (with full details)\n",
    "\t* Record secondary performance measures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Include libraries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get general systems"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "import json\n",
    "from datetime import datetime\n",
    "from copy import deepcopy\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data management and plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PyTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n",
    "cuda_device = torch.device(\"cuda:1\")\n",
    "\n",
    "assert device.type == 'cuda'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets, metrics\n",
    "from sklearn.preprocessing import StandardScaler, normalize, OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split, KFold\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sklearn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set general parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_SEED      = 42\n",
    "NUMBER_OF_SPLITS = 10\n",
    "SAVE_FILE_NAME   = './../../../data/uci/processed/results/adult/results_{}.csv'.format(datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset\n",
    "\n",
    "The dataset is Adult UCI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_columns = [\n",
    "    \"age\", \n",
    "    \"workclass\",\n",
    "    \"fnlwgt\", \n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"gt50k\"]\n",
    "y_cols = 'gt50k'\n",
    "\n",
    "\n",
    "data = pd.read_csv('./../../../data/uci/processed/data/adult/adult.data',\n",
    "                   names=adult_columns,\n",
    "                  index_col=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split into Xs and ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = data.columns.values.tolist()\n",
    "x_cols.remove(y_cols)\n",
    "\n",
    "xs_raw = data[x_cols]\n",
    "ys_raw = data[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_mask = xs_raw.dtypes==object\n",
    "numerical_feature_mask = xs_raw.dtypes==\"int64\"\n",
    "\n",
    "categorical_cols = xs_raw.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols = xs_raw.columns[numerical_feature_mask].tolist()\n",
    "\n",
    "scaler = StandardScaler()\n",
    "xs = xs_raw.copy()\n",
    "\n",
    "# OHE categoricals\n",
    "onehotencoded = pd.get_dummies(xs_raw[categorical_cols])\n",
    "xs[onehotencoded.columns] = onehotencoded\n",
    "xs = xs.drop(categorical_cols, axis=1)\n",
    "\n",
    "## Linear scaling\n",
    "numericals = xs_raw[numerical_cols].values #returns a numpy array\n",
    "scaler = StandardScaler()\n",
    "numericals = scaler.fit_transform(xs_raw[numerical_cols].values)\n",
    "xs[numerical_cols] = pd.DataFrame(numericals)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "## Adjust outcome var\n",
    "ys = data['gt50k'] == ' >50K'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Results structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Results are going to have:\n",
    "* What type of model\n",
    "* What random seed\n",
    "* What cross fold\n",
    "* What performance measures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mark_result(model_name, random_seed, cross_fold_index, predictions, AUC_score):\n",
    "    return {\n",
    "        'modelName': model_name,\n",
    "        'randomSeed': random_seed,\n",
    "        'crossFoldIndex': cross_fold_index,\n",
    "        'predictions': list(predictions),\n",
    "        'auc':AUC_score\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits = NUMBER_OF_SPLITS, \n",
    "           random_state = RANDOM_SEED,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetLarge(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_width=256):\n",
    "        super(NeuralNetLarge, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_width) \n",
    "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc3 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc4 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc5 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc6 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc7 = nn.Linear(hidden_width, output_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc6(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc7(out)\n",
    "        return out\n",
    "    \n",
    "class NeuralNetMed(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_width=128):\n",
    "        super(NeuralNetMed, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_width) \n",
    "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc3 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc4 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc5 = nn.Linear(hidden_width, output_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "    \n",
    "class NeuralNetSmall(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_width=32):\n",
    "        super(NeuralNetSmall, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_width) \n",
    "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc3 = nn.Linear(hidden_width, output_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "    \n",
    "    \n",
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs.iloc[idx].to_numpy()\n",
    "        y = 1 if self.ys.iloc[idx] else 0\n",
    "        return (x, y)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Currently training 0\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4147\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3598\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3151\n",
      "Epoch [4/25], Step [29/29], Loss: 0.3301\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2921\n",
      "Epoch [6/25], Step [29/29], Loss: 0.3333\n",
      "Epoch [7/25], Step [29/29], Loss: 0.2719\n",
      "Epoch [8/25], Step [29/29], Loss: 0.3362\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3291\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3572\n",
      "Epoch [11/25], Step [29/29], Loss: 0.3014\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2822\n",
      "Epoch [13/25], Step [29/29], Loss: 0.3126\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2893\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2720\n",
      "Epoch [16/25], Step [29/29], Loss: 0.2834\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2606\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2569\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2402\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2570\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2735\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2709\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2147\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2428\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2434\n",
      "0.9027991273123579\n",
      "Currently training 1\n",
      "Epoch [1/25], Step [29/29], Loss: 0.3838\n",
      "Epoch [2/25], Step [29/29], Loss: 0.2997\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3275\n",
      "Epoch [4/25], Step [29/29], Loss: 0.2988\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2763\n",
      "Epoch [6/25], Step [29/29], Loss: 0.2909\n",
      "Epoch [7/25], Step [29/29], Loss: 0.2834\n",
      "Epoch [8/25], Step [29/29], Loss: 0.2716\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3200\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3325\n",
      "Epoch [11/25], Step [29/29], Loss: 0.2696\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2567\n",
      "Epoch [13/25], Step [29/29], Loss: 0.2742\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2592\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2912\n",
      "Epoch [16/25], Step [29/29], Loss: 0.2907\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2397\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2968\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2665\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2656\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2708\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2672\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2211\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2166\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2878\n",
      "0.888508635371065\n",
      "Currently training 2\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4395\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3203\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3168\n",
      "Epoch [4/25], Step [29/29], Loss: 0.3105\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2892\n",
      "Epoch [6/25], Step [29/29], Loss: 0.3189\n",
      "Epoch [7/25], Step [29/29], Loss: 0.2725\n",
      "Epoch [8/25], Step [29/29], Loss: 0.3215\n",
      "Epoch [9/25], Step [29/29], Loss: 0.2746\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3170\n",
      "Epoch [11/25], Step [29/29], Loss: 0.2669\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2849\n",
      "Epoch [13/25], Step [29/29], Loss: 0.3081\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2585\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2627\n",
      "Epoch [16/25], Step [29/29], Loss: 0.2699\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2496\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2928\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2855\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2627\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2786\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2391\n",
      "Epoch [23/25], Step [29/29], Loss: 0.1921\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2687\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2563\n",
      "0.8773634168273832\n",
      "Currently training 3\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4241\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3207\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3199\n",
      "Epoch [4/25], Step [29/29], Loss: 0.2920\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2932\n",
      "Epoch [6/25], Step [29/29], Loss: 0.3144\n",
      "Epoch [7/25], Step [29/29], Loss: 0.3233\n",
      "Epoch [8/25], Step [29/29], Loss: 0.2833\n",
      "Epoch [9/25], Step [29/29], Loss: 0.2817\n",
      "Epoch [10/25], Step [29/29], Loss: 0.2868\n",
      "Epoch [11/25], Step [29/29], Loss: 0.2641\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2783\n",
      "Epoch [13/25], Step [29/29], Loss: 0.2556\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2687\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2877\n",
      "Epoch [16/25], Step [29/29], Loss: 0.3083\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2441\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2781\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2380\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2742\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2492\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2459\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2434\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2722\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2165\n",
      "0.8988232280617166\n",
      "Currently training 4\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4114\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3441\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3338\n",
      "Epoch [4/25], Step [29/29], Loss: 0.2919\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2998\n",
      "Epoch [6/25], Step [29/29], Loss: 0.2862\n",
      "Epoch [7/25], Step [29/29], Loss: 0.2966\n",
      "Epoch [8/25], Step [29/29], Loss: 0.2915\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3341\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3087\n",
      "Epoch [11/25], Step [29/29], Loss: 0.2764\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2968\n",
      "Epoch [13/25], Step [29/29], Loss: 0.2850\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2833\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2879\n",
      "Epoch [16/25], Step [29/29], Loss: 0.2399\n",
      "Epoch [17/25], Step [29/29], Loss: 0.3053\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2478\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2444\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2531\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2405\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2143\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2190\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2256\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2359\n",
      "0.9021873298935086\n",
      "Currently training 5\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4268\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3376\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3221\n",
      "Epoch [4/25], Step [29/29], Loss: 0.3097\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2791\n",
      "Epoch [6/25], Step [29/29], Loss: 0.2962\n",
      "Epoch [7/25], Step [29/29], Loss: 0.3232\n",
      "Epoch [8/25], Step [29/29], Loss: 0.3012\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3005\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3180\n",
      "Epoch [11/25], Step [29/29], Loss: 0.2995\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2460\n",
      "Epoch [13/25], Step [29/29], Loss: 0.2601\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2959\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2547\n",
      "Epoch [16/25], Step [29/29], Loss: 0.3079\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2339\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2980\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2667\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2899\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2764\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2369\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2441\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2200\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2369\n",
      "0.9001174350395106\n",
      "Currently training 6\n",
      "Epoch [1/25], Step [29/29], Loss: 0.4295\n",
      "Epoch [2/25], Step [29/29], Loss: 0.3557\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3041\n",
      "Epoch [4/25], Step [29/29], Loss: 0.2845\n",
      "Epoch [5/25], Step [29/29], Loss: 0.2945\n",
      "Epoch [6/25], Step [29/29], Loss: 0.3461\n",
      "Epoch [7/25], Step [29/29], Loss: 0.2888\n",
      "Epoch [8/25], Step [29/29], Loss: 0.2787\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3334\n",
      "Epoch [10/25], Step [29/29], Loss: 0.3215\n",
      "Epoch [11/25], Step [29/29], Loss: 0.3041\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2730\n",
      "Epoch [13/25], Step [29/29], Loss: 0.3084\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2692\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2672\n",
      "Epoch [16/25], Step [29/29], Loss: 0.2613\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2643\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2523\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2619\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2225\n",
      "Epoch [21/25], Step [29/29], Loss: 0.2278\n",
      "Epoch [22/25], Step [29/29], Loss: 0.2826\n",
      "Epoch [23/25], Step [29/29], Loss: 0.2396\n",
      "Epoch [24/25], Step [29/29], Loss: 0.2427\n",
      "Epoch [25/25], Step [29/29], Loss: 0.2294\n",
      "0.8890402340432224\n",
      "Currently training 7\n",
      "Epoch [1/25], Step [29/29], Loss: 0.3852\n",
      "Epoch [2/25], Step [29/29], Loss: 0.2993\n",
      "Epoch [3/25], Step [29/29], Loss: 0.3243\n",
      "Epoch [4/25], Step [29/29], Loss: 0.3189\n",
      "Epoch [5/25], Step [29/29], Loss: 0.3101\n",
      "Epoch [6/25], Step [29/29], Loss: 0.3039\n",
      "Epoch [7/25], Step [29/29], Loss: 0.3193\n",
      "Epoch [8/25], Step [29/29], Loss: 0.2784\n",
      "Epoch [9/25], Step [29/29], Loss: 0.3326\n",
      "Epoch [10/25], Step [29/29], Loss: 0.2816\n",
      "Epoch [11/25], Step [29/29], Loss: 0.3065\n",
      "Epoch [12/25], Step [29/29], Loss: 0.2906\n",
      "Epoch [13/25], Step [29/29], Loss: 0.2635\n",
      "Epoch [14/25], Step [29/29], Loss: 0.2804\n",
      "Epoch [15/25], Step [29/29], Loss: 0.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [16/25], Step [29/29], Loss: 0.2765\n",
      "Epoch [17/25], Step [29/29], Loss: 0.2647\n",
      "Epoch [18/25], Step [29/29], Loss: 0.2435\n",
      "Epoch [19/25], Step [29/29], Loss: 0.2837\n",
      "Epoch [20/25], Step [29/29], Loss: 0.2714\n"
     ]
    }
   ],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    \n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 25\n",
    "\n",
    "    train_data = TabularDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True)\n",
    "\n",
    "    validate_data = TabularDataset(X_test.reset_index(drop=True), y_test.reset_index(drop=True))\n",
    "    validate_loader = DataLoader(dataset = validate_data,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=False)\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    my_random_seed = 42\n",
    "    random.seed(my_random_seed)\n",
    "    nn_model = NeuralNetLarge(108, 1).to(cuda_device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss().to(cuda_device)\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (xsnn, ysnn) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            xsnn = xsnn.float().to(cuda_device)\n",
    "            ysnn = ysnn.view(-1, 1).float().to(cuda_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nn_model(xsnn)\n",
    "            train_loss = criterion(outputs, ysnn)\n",
    "\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))\n",
    "\n",
    "            \n",
    "    nn_preds = torch.sigmoid( nn_model.forward(torch.from_numpy(X_test.to_numpy()).float().to(cuda_device)).to(device)).detach().cpu().numpy()\n",
    "    nn_preds = nn_preds.reshape(nn_preds.shape[0])\n",
    "    auc = roc_auc_score(y_test, nn_preds)\n",
    "    print(auc)\n",
    "    results.append(mark_result('NN Large', RANDOM_SEED, index, nn_preds, auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Med"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    \n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 10\n",
    "\n",
    "    train_data = TabularDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True)\n",
    "\n",
    "    validate_data = TabularDataset(X_test.reset_index(drop=True), y_test.reset_index(drop=True))\n",
    "    validate_loader = DataLoader(dataset = validate_data,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=False)\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    my_random_seed = 42\n",
    "    random.seed(my_random_seed)\n",
    "    nn_model = NeuralNetLarge(108, 1).to(cuda_device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss().to(cuda_device)\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (xsnn, ysnn) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            xsnn = xsnn.float().to(cuda_device)\n",
    "            ysnn = ysnn.view(-1, 1).float().to(cuda_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nn_model(xsnn)\n",
    "            train_loss = criterion(outputs, ysnn)\n",
    "\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))\n",
    "\n",
    "            \n",
    "    nn_preds = torch.sigmoid( nn_model.forward(torch.from_numpy(X_test.to_numpy()).float().to(cuda_device)).to(device)).detach().cpu().numpy()\n",
    "    nn_preds = nn_preds.reshape(nn_preds.shape[0])\n",
    "    auc = roc_auc_score(y_test, nn_preds)\n",
    "    print(auc)\n",
    "    results.append(mark_result('NN Med', RANDOM_SEED, index, nn_preds, auc))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NN Small"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    \n",
    "    batch_size = 1024\n",
    "    learning_rate = 0.0005\n",
    "    num_epochs = 10\n",
    "\n",
    "    train_data = TabularDataset(X_train, y_train)\n",
    "    train_loader = DataLoader(train_data, \n",
    "                               batch_size=batch_size, \n",
    "                               shuffle=True)\n",
    "\n",
    "    validate_data = TabularDataset(X_test.reset_index(drop=True), y_test.reset_index(drop=True))\n",
    "    validate_loader = DataLoader(dataset = validate_data,\n",
    "                                 batch_size=batch_size, \n",
    "                                 shuffle=False)\n",
    "\n",
    "    total_step = len(train_loader)\n",
    "\n",
    "    my_random_seed = 42\n",
    "    random.seed(my_random_seed)\n",
    "    nn_model = NeuralNetSmall(108, 1).to(cuda_device)\n",
    "\n",
    "    criterion = nn.BCEWithLogitsLoss().to(cuda_device)\n",
    "    optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)  \n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (xsnn, ysnn) in enumerate(train_loader):  \n",
    "            # Move tensors to the configured device\n",
    "            xsnn = xsnn.float().to(cuda_device)\n",
    "            ysnn = ysnn.view(-1, 1).float().to(cuda_device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = nn_model(xsnn)\n",
    "            train_loss = criterion(outputs, ysnn)\n",
    "\n",
    "\n",
    "            # Backward and optimize\n",
    "            optimizer.zero_grad()\n",
    "            train_loss.backward()\n",
    "            optimizer.step()\n",
    "        if (epoch+1) % 1 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))\n",
    "\n",
    "            \n",
    "    nn_preds = torch.sigmoid( nn_model.forward(torch.from_numpy(X_test.to_numpy()).float().to(cuda_device)).to(device)).detach().cpu().numpy()\n",
    "    nn_preds = nn_preds.reshape(nn_preds.shape[0])\n",
    "    auc = roc_auc_score(y_test, nn_preds)\n",
    "    print(auc)\n",
    "    results.append(mark_result('NN Small', RANDOM_SEED, index, nn_preds, auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nn_preds = torch.sigmoid( nn_model.forward(torch.from_numpy(X_test.to_numpy()).float().to(cuda_device)).to(device)).detach().cpu().numpy()\n",
    "# nn_preds = nn_preds.reshape(nn_preds.shape[0])\n",
    "# auc = roc_auc_score(y_test, nn_preds)\n",
    "# results.append(mark_result('NN', RANDOM_SEED, index, nn_preds, auc))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    # Instantiate model with 1000 decision trees\n",
    "    rf = RandomForestRegressor(n_estimators = 100, random_state = 42)\n",
    "    # Train the model on training data\n",
    "    rf.fit(X_train, y_train)\n",
    "    # Use the forest's predict method on the test data\n",
    "    rf_preds = rf.predict(X_test)\n",
    "    # Calculate the absolute errors\n",
    "    errors = abs(rf_preds - y_test)\n",
    "    auc = roc_auc_score(y_test, rf_preds)\n",
    "    results.append(mark_result('Random Forest', RANDOM_SEED, index, rf_preds, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    \n",
    "    svm_model=SVC()\n",
    "    svm_model.fit(X_train, y_train)\n",
    "    svm_preds=svm_model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, svm_preds)\n",
    "\n",
    "    results.append(mark_result('SVM', RANDOM_SEED, index, svm_preds, auc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    X_train, X_test = xs.iloc[train_index], xs.iloc[test_index]\n",
    "    y_train, y_test = ys[train_index], ys[test_index]\n",
    "    \n",
    "    regression_model=LinearRegression()\n",
    "    regression_model.fit(X_train, y_train)\n",
    "    regression_preds=regression_model.predict(X_test)\n",
    "    auc = roc_auc_score(y_test, regression_preds)\n",
    "\n",
    "    results.append(mark_result('Regression', RANDOM_SEED, index, regression_preds, auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "if not os.path.exists('./../../../data/uci/processed/results/adult'):\n",
    "    os.makedirs('./../../../data/uci/processed/results/adult')\n",
    "# with open(SAVE_FILE_NAME, 'w') as fp:\n",
    "#     json.dump(results, fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.to_csv(SAVE_FILE_NAME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "res_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_df.groupby('modelName').aggregate(['mean', 'std'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for index, (train_index, test_index) in enumerate(kf.split(xs)):\n",
    "    print(\"Currently training {}\".format(index))\n",
    "    print(test_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
