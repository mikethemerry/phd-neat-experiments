{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import neat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "\n",
    "# from explaneat.core.backprop import NeatNet\n",
    "# from explaneat.core import backprop\n",
    "# from explaneat.core.backproppop import BackpropPopulation\n",
    "# from explaneat.visualization import visualize\n",
    "# from explaneat.core.experiment import ExperimentReporter\n",
    "# from explaneat.core.utility import one_hot_encode\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# USE_CUDA = False\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n",
    "cuda_device = torch.device(\"cuda:1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iris Experiment\n",
    "\n",
    "This experiment (a) test the experimental environment, but is also to evaluate the efficacy of the ExplaNEAT algorithm. Speed is a critical factor, as well as stability of results on population size. Total run time will also be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a random seed and a total stopping point in the number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_seed = 42\n",
    "random.seed(my_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vals):\n",
    "    width = max(vals)\n",
    "    newVals = []\n",
    "    for val in vals:\n",
    "        blank = [0. for _ in range(width + 1)]\n",
    "        blank[val] = 1.\n",
    "        newVals.append(blank)\n",
    "    return np.asarray(newVals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to work with the Iris dataset, which will be loaded from `sklearn`. We want to characterise the efficacy of the algorithm with regards to a mostly untransformed dataset, so we will only normalise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "adult_columns = [\n",
    "    \"age\", \n",
    "    \"workclass\",\n",
    "    \"fnlwgt\", \n",
    "    \"education\",\n",
    "    \"education_num\",\n",
    "    \"marital_status\",\n",
    "    \"occupation\",\n",
    "    \"relationship\",\n",
    "    \"race\",\n",
    "    \"sex\",\n",
    "    \"capital_gain\",\n",
    "    \"capital_loss\",\n",
    "    \"hours_per_week\",\n",
    "    \"native_country\",\n",
    "    \"gt50k\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('./../../../data/uci/processed/data/adult/adult.data',\n",
    "                   names=adult_columns,\n",
    "                  index_col=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data.dtypes    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, OneHotEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = data.columns.values.tolist()\n",
    "y_cols = 'gt50k'\n",
    "x_cols.remove(y_cols)\n",
    "\n",
    "xs_raw = data[x_cols]\n",
    "ys_raw = data[y_cols]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_feature_mask = xs_raw.dtypes==object\n",
    "numerical_feature_mask = xs_raw.dtypes==\"int64\"\n",
    "\n",
    "categorical_cols = xs_raw.columns[categorical_feature_mask].tolist()\n",
    "numerical_cols = xs_raw.columns[numerical_feature_mask].tolist()\n",
    "\n",
    "scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xs_raw.copy()\n",
    "\n",
    "# OHE categoricals\n",
    "onehotencoded = pd.get_dummies(xs_raw[categorical_cols])\n",
    "xs[onehotencoded.columns] = onehotencoded\n",
    "xs = xs.drop(categorical_cols, axis=1)\n",
    "\n",
    "## Linear scaling\n",
    "numericals = xs_raw[numerical_cols].values #returns a numpy array\n",
    "scaler = StandardScaler()\n",
    "numericals = scaler.fit_transform(xs_raw[numerical_cols].values)\n",
    "xs[numerical_cols] = pd.DataFrame(numericals)\n",
    "\n",
    "\n",
    "####\n",
    "\n",
    "## Adjust outcome var\n",
    "ys = data['gt50k'] == ' >50K'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(xs, ys, test_size=0.15, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the data we are working with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "ys[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "The NEAT implementation on which ExplaNEAT extends uses a single function call for evaluating fitness. Although this might be reworked for ExplaNEAT to be able to get consistency between the genome-evaluation and the backprop loss function, that can be reviewed later.\n",
    "\n",
    "This use `CrossEntropyLoss` from `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_genomes(genomes, config):\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    loss = loss.to(device)\n",
    "\n",
    "    for genome_id, genome in genomes:\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        preds = []\n",
    "        for xi in X_train:\n",
    "            preds.append(net.activate(xi))\n",
    "        genome.fitness = float(1./loss(torch.tensor(preds).to(device), torch.tensor(y_train)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The competition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the model we are using\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "# Instantiate model with 1000 decision trees\n",
    "rf = RandomForestRegressor(n_estimators = 2500, random_state = 42)\n",
    "# Train the model on training data\n",
    "rf.fit(X_train, y_train);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the forest's predict method on the test data\n",
    "rf_preds = rf.predict(X_test)\n",
    "# Calculate the absolute errors\n",
    "errors = abs(rf_preds - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Mean Absolute Error:', round(np.mean(errors), 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "errors = abs(rf_preds - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "abs(rf_preds.round(0) - y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the classification report and confusion matrix\n",
    "print(confusion_matrix(y_test,rf_preds.round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, rf.predict(X_train).round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, rf_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "svm_model=SVC()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_model.fit(X_train, y_train)\n",
    "svm_preds=svm_model.predict(X_test)\n",
    "# print(confusion_matrix(y_test,pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the classification report and confusion matrix\n",
    "print(confusion_matrix(y_test,svm_preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, svm_model.predict(X_train).round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, svm_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "regression_model=LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_preds=regression_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the classification report and confusion matrix\n",
    "print(confusion_matrix(y_test,regression_preds.round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(confusion_matrix(y_train, regression_model.predict(X_train).round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, regression_preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_width=64):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_width) \n",
    "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc3 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc4 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc5 = nn.Linear(hidden_width, output_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out\n",
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return (x, y)\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 1000\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 30"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = TabularDataset(X_train, y_train)\n",
    "train_loader = DataLoader(train_data, \n",
    "                           batch_size=batch_size, \n",
    "                           shuffle=True)\n",
    "\n",
    "validate_data = TabularDataset(X_test, y_test)\n",
    "validate_loader = DataLoader(dataset = validate_data,\n",
    "                             batch_size=batch_size, \n",
    "                             shuffle=False)\n",
    "\n",
    "total_step = len(train_loader)\n",
    "\n",
    "my_random_seed = 42\n",
    "random.seed(my_random_seed)\n",
    "nn_model = NeuralNet(9, 1).to(cuda_device)\n",
    "\n",
    "criterion = nn.BCEWithLogitsLoss().to(cuda_device)\n",
    "optimizer = torch.optim.Adam(nn_model.parameters(), lr=learning_rate)  \n",
    "for epoch in range(num_epochs):\n",
    "    for i, (xsnn, ysnn) in enumerate(train_loader):  \n",
    "        # Move tensors to the configured device\n",
    "        xsnn = xsnn.float().to(cuda_device)\n",
    "        ysnn = ysnn.view(-1, 1).float().to(cuda_device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = nn_model(xsnn)\n",
    "        train_loss = criterion(outputs, ysnn)\n",
    "        \n",
    "\n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        if (epoch+1) % 50 == 0:\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                   .format(epoch+1, num_epochs, i+1, total_step, train_loss.item()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn_preds = torch.sigmoid( nn_model.forward(torch.from_numpy(X_test).float().to(cuda_device)).to(device)).detach().numpy()\n",
    "nn_roc_score = roc_auc_score(y_test, nn_preds)\n",
    "nn_roc_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ExplaNEAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "filePath = './../../data/experiments/bchard/experiment-longepochsttsplit-10-0/fullStatus.xplnt'\n",
    "with gzip.open(filePath, 'rb') as f: \n",
    "    data = pickle.load(f)     \n",
    "    \n",
    "p, g, ancestry, ancestors, randomState = data\n",
    "config = p.config\n",
    "bestNet = neat.nn.FeedForwardNetwork.create(g, config)\n",
    "\n",
    "explaneat_preds = [bestNet.activate(x)[0] for x in X_test]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_max_depth(genome):\n",
    "    connections = {}\n",
    "    def my_depth(connections, ix):\n",
    "        if ix < 0:\n",
    "            return 0\n",
    "        depths = []\n",
    "        for c in connections[ix]:\n",
    "            depths.append(my_depth(connections, c))\n",
    "        return max(depths) + 1\n",
    "                \n",
    "    for key, conn in  genome.connections.items():\n",
    "        if not key[1] in connections:\n",
    "            connections[key[1]] = []\n",
    "        connections[key[1]].append(key[0])\n",
    "    all_depths = {}\n",
    "    for c in connections:\n",
    "        all_depths[c] = my_depth(connections, c)\n",
    "    return all_depths\n",
    "print(g.size())\n",
    "print(calculate_max_depth(g))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "config.genome_config.num_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_input_connections(genome, config):\n",
    "    n_inputs = config.genome_config.num_inputs\n",
    "    checks = {-n: False for n in range(1, n_inputs+1)}\n",
    "    for c in genome.connections:\n",
    "        if c[0] < 0:\n",
    "            checks[c[0]] = True\n",
    "    print(checks)\n",
    "    mySum = 0\n",
    "    for k, v in checks.items():\n",
    "        if v: \n",
    "            mySum += 1\n",
    "    return mySum\n",
    "check_input_connections(g, config)\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the classification report and confusion matrix\n",
    "print(confusion_matrix(y_test,np.array(explaneat_preds).round(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "roc_auc_score(y_test, explaneat_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_preds = {\n",
    "    'svm': svm_model.predict(X_train),\n",
    "    'regression': regression_model.predict(X_train),\n",
    "    'rf': rf.predict(X_train),\n",
    "    'ExplaNEAT': [bestNet.activate(x)[0] for x in X_train],\n",
    "    'NN': torch.sigmoid( nn_model.forward(torch.from_numpy(X_train).float().to(cuda_device)).to(device)).detach().numpy()\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for model, preds in train_preds.items():\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_train, preds)\n",
    "    auc = metrics.roc_auc_score(y_train, preds)\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area=%0.3f)' % (model, auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_preds = {\n",
    "    'svm': svm_model.predict(X_test),\n",
    "    'regression': regression_model.predict(X_test),\n",
    "    'rf': rf.predict(X_test),\n",
    "    'ExplaNEAT': [bestNet.activate(x)[0] for x in X_test],\n",
    "    'NN': torch.sigmoid( nn_model.forward(torch.from_numpy(X_test).float().to(cuda_device)).to(device)).detach().numpy()\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for model, preds in test_preds.items():\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n",
    "    auc = metrics.roc_auc_score(y_test, preds)\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area=%0.3f)' % (model, auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_preds = {\n",
    "    'svm': svm_preds,\n",
    "    'regression': regression_preds,\n",
    "    'rf': rf_preds,\n",
    "    'ExplaNEAT': explaneat_preds,\n",
    "    'NN': torch.sigmoid( nn_model.forward(torch.from_numpy(X_test).float().to(cuda_device)).to(device)).detach().numpy()\n",
    "}\n",
    "\n",
    "plt.figure()\n",
    "for model, preds in all_preds.items():\n",
    "    fpr, tpr, thresholds = metrics.roc_curve(y_test, preds)\n",
    "    auc = metrics.roc_auc_score(y_test, preds)\n",
    "    plt.plot(fpr, tpr, label='%s ROC (area=%0.3f)' % (model, auc))\n",
    "# Custom settings for the plot \n",
    "plt.plot([0, 1], [0, 1],'r--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('1-Specificity(False Positive Rate)')\n",
    "plt.ylabel('Sensitivity(True Positive Rate)')\n",
    "plt.title('Receiver Operating Characteristic')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()   # Display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "for model, preds in all_preds.items():\n",
    "    print(model)\n",
    "    print(len(preds))\n",
    "    print(metrics.roc_auc_score(y_test, preds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}