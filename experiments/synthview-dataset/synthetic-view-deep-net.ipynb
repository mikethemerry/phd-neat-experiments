{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import neat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n",
    "from explaneat.core.backprop import NeatNet\n",
    "from explaneat.core import backprop\n",
    "from explaneat.core.backproppop import BackpropPopulation\n",
    "from explaneat.visualization import visualize\n",
    "from explaneat.core.experiment import ExperimentReporter\n",
    "from explaneat.core.utility import one_hot_encode\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "# USE_CUDA = False\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Experiment\n",
    "\n",
    "This experiment (a) test the experimental environment, but is also to evaluate the efficacy of the ExplaNEAT algorithm. Speed is a critical factor, as well as stability of results on population size. Total run time will also be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a random seed and a total stopping point in the number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_seed = 42\n",
    "random.seed(my_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vals):\n",
    "    width = max(vals)\n",
    "    newVals = []\n",
    "    for val in vals:\n",
    "        blank = [0. for _ in range(width + 1)]\n",
    "        blank[val] = 1.\n",
    "        newVals.append(blank)\n",
    "    return np.asarray(newVals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to work with the Iris dataset, which will be loaded from `sklearn`. We want to characterise the efficacy of the algorithm with regards to a mostly untransformed dataset, so we will only normalise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fp, \n",
    "                 randomSeed = 42, \n",
    "                 proportionValidation = 0.2):\n",
    "    ''' Takes in a filepath, returns x_train, x_validate, y_train, y_validate'''\n",
    "    df = pd.read_csv(fp).reset_index(drop=True)\n",
    "    xs_raw = df[[\n",
    "        'ag_age',\n",
    "        'ag_sex',\n",
    "        'ag_eth',\n",
    "        'pt_nzdep',\n",
    "        'imp_hxdiab',\n",
    "        'pt_tc_hdl_ratio',\n",
    "        'pt_bps',\n",
    "        'pt_bpd',\n",
    "        'pt_smoke',\n",
    "        'imp_hxcvd',\n",
    "        'imp_hdl',\n",
    "        'imp_ldl',\n",
    "        'imp_tchol',\n",
    "        'marker',\n",
    "        'region',\n",
    "        'PH_BL_LLD_ANY',\n",
    "        'PH_BL_AHT_ANY',\n",
    "        'pt_familyhistory',\n",
    "        'ab_gen',\n",
    "        'eth_gen',\n",
    "        'is.female',\n",
    "        'log.age',\n",
    "        'log.age.gender',\n",
    "        'log.sbp',\n",
    "        'smoking',\n",
    "        'log.tchdl',\n",
    "        'diabetes',\n",
    "        'diabetes.sex']]\n",
    "    \n",
    "    xs_raw = xs_raw[[\n",
    "        'is.female', \n",
    "        'ag_age',\n",
    "        'pt_bps',\n",
    "        'smoking',\n",
    "        'pt_tc_hdl_ratio',\n",
    "        'diabetes'\n",
    "    ]]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xs_raw)\n",
    "    xs = scaler.transform(xs_raw)\n",
    "    ys = df['dead'].apply(lambda x: 1 if x else 0)\n",
    "    ys = np.array(ys).astype(float)\n",
    "    if proportionValidation == 0:\n",
    "        return xs, [], ys, []\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(xs, ys, test_size=proportionValidation, random_state=randomSeed)\n",
    "    return X_train, X_validate, y_train, y_validate\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, _, y_test, __ = load_dataset('./../../data/processed/synthetic_view/synthetic_view_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.04660448, -0.79372918, -0.82597264, -0.86593193, -2.14787211,\n",
       "        -0.3018414 ],\n",
       "       [-1.04660448, -1.54731757,  0.1902044 ,  1.15482518, -0.0411511 ,\n",
       "        -0.3018414 ],\n",
       "       [-1.04660448,  0.51355442,  0.03688167,  1.15482518, -0.10389362,\n",
       "        -0.3018414 ],\n",
       "       [ 0.95547078, -0.86687438, -1.44050974, -0.86593193, -0.10620131,\n",
       "        -0.3018414 ],\n",
       "       [ 0.95547078,  0.99632167, -0.82704809, -0.86593193, -0.8058885 ,\n",
       "        -0.3018414 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "The NEAT implementation on which ExplaNEAT extends uses a single function call for evaluating fitness. Although this might be reworked for ExplaNEAT to be able to get consistency between the genome-evaluation and the backprop loss function, that can be reviewed later.\n",
    "\n",
    "This use `Binary Cross Entropy Loss` from `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_genomes(genomes, config):\n",
    "#     loss = nn.BCELoss()\n",
    "#     loss = loss.to(device)\n",
    "#     for genome_id, genome in genomes:\n",
    "#         net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "#         preds = []\n",
    "#         for xi in X_validate:\n",
    "#             preds.append(1. if net.activate(xi)[0] > 0.5 else 0.)\n",
    "#         correct = 0\n",
    "#         for pred, truth in zip(preds, y_validate):\n",
    "#             if pred == truth:\n",
    "#                 correct += 1.\n",
    "        \n",
    "        \n",
    "#         genome.fitness = float(correct / len(preds))\n",
    "def eval_genomes(genomes, config):\n",
    "    loss = nn.BCEWithLogitsLoss()\n",
    "    loss = loss.to(device)\n",
    "    for genome_id, genome in genomes:\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        preds = []\n",
    "        for xi in X_validate:\n",
    "            preds.append(net.activate(xi))\n",
    "        genome.fitness = float(1./loss(torch.tensor(preds), torch.tensor(y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a neural network\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNet(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_width=64):\n",
    "        super(NeuralNet, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_size, hidden_width) \n",
    "        self.fc2 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc3 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc4 = nn.Linear(hidden_width, hidden_width)\n",
    "        self.fc5 = nn.Linear(hidden_width, output_size)  \n",
    "        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc3(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc4(out)\n",
    "        out = self.relu(out)\n",
    "        out = self.fc5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to put a hard limit on how long this can go on for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nIterations = 5\n",
    "nEpochs = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Vary dataset size\n",
    "\n",
    "The first experiment is going to examine the difference from different dataset sizess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetSizes = [\n",
    "        1000,\n",
    "        2500,\n",
    "        5000,\n",
    "        10000,\n",
    "        25000,\n",
    "        50000,\n",
    "        100000,\n",
    "        250000,\n",
    "#         500000,\n",
    "#         1000000,\n",
    "#         1500000,\n",
    "#         2000000\n",
    "    ]\n",
    "# datasetSizes = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocationTemplate = './../../data/experiments/synthview-nn/experiment-dataset-{}-{}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLocation = './../../data/processed/synthetic_view/'\n",
    "datasetFileTemplate = 'synthetic_view_test_{:07d}.csv'\n",
    "# os.path.join(output_filepath, 'synthetic_view_test_{:07d}.csv'.format(dsSize)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 500\n",
    "learning_rate = 0.0005\n",
    "num_epochs = 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TabularDataset(Dataset):\n",
    "    \"\"\"Face Landmarks dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, xs, ys):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            csv_file (string): Path to the csv file with annotations.\n",
    "            root_dir (string): Directory with all the images.\n",
    "            transform (callable, optional): Optional transform to be applied\n",
    "                on a sample.\n",
    "        \"\"\"\n",
    "        self.xs = xs\n",
    "        self.ys = ys\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.xs)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        x = self.xs[idx]\n",
    "        y = self.ys[idx]\n",
    "        return (x, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Step [2/2], Loss: 0.2909\n",
      "Epoch [200/5000], Step [2/2], Loss: 0.2763\n",
      "Epoch [300/5000], Step [2/2], Loss: 0.2294\n",
      "Epoch [400/5000], Step [2/2], Loss: 0.2511\n",
      "Epoch [500/5000], Step [2/2], Loss: 0.2378\n",
      "Epoch [600/5000], Step [2/2], Loss: 0.2131\n",
      "Epoch [700/5000], Step [2/2], Loss: 0.2053\n",
      "Epoch [800/5000], Step [2/2], Loss: 0.2419\n",
      "Epoch [900/5000], Step [2/2], Loss: 0.2692\n",
      "Epoch [1000/5000], Step [2/2], Loss: 0.1669\n",
      "Epoch [1100/5000], Step [2/2], Loss: 0.2014\n",
      "Epoch [1200/5000], Step [2/2], Loss: 0.1745\n",
      "Epoch [1300/5000], Step [2/2], Loss: 0.2084\n",
      "Epoch [1400/5000], Step [2/2], Loss: 0.1708\n",
      "Epoch [1500/5000], Step [2/2], Loss: 0.1888\n",
      "Epoch [1600/5000], Step [2/2], Loss: 0.1962\n",
      "Epoch [1700/5000], Step [2/2], Loss: 0.2307\n",
      "Epoch [1800/5000], Step [2/2], Loss: 0.2168\n",
      "Epoch [1900/5000], Step [2/2], Loss: 0.1824\n",
      "Epoch [2000/5000], Step [2/2], Loss: 0.1793\n",
      "Epoch [2100/5000], Step [2/2], Loss: 0.2040\n",
      "Epoch [2200/5000], Step [2/2], Loss: 0.1607\n",
      "Epoch [2300/5000], Step [2/2], Loss: 0.1825\n",
      "Epoch [2400/5000], Step [2/2], Loss: 0.1742\n",
      "Epoch [2500/5000], Step [2/2], Loss: 0.1954\n",
      "Epoch [2600/5000], Step [2/2], Loss: 0.1771\n",
      "Epoch [2700/5000], Step [2/2], Loss: 0.1609\n",
      "Epoch [2800/5000], Step [2/2], Loss: 0.1735\n",
      "Epoch [2900/5000], Step [2/2], Loss: 0.1283\n",
      "Epoch [3000/5000], Step [2/2], Loss: 0.1556\n",
      "Epoch [3100/5000], Step [2/2], Loss: 0.1616\n",
      "Epoch [3200/5000], Step [2/2], Loss: 0.1479\n",
      "Epoch [3300/5000], Step [2/2], Loss: 0.1102\n",
      "Epoch [3400/5000], Step [2/2], Loss: 0.1523\n",
      "Epoch [3500/5000], Step [2/2], Loss: 0.1258\n",
      "Epoch [3600/5000], Step [2/2], Loss: 0.1532\n",
      "Epoch [3700/5000], Step [2/2], Loss: 0.1363\n",
      "Epoch [3800/5000], Step [2/2], Loss: 0.1181\n",
      "Epoch [3900/5000], Step [2/2], Loss: 0.1327\n",
      "Epoch [4000/5000], Step [2/2], Loss: 0.1332\n",
      "Epoch [4100/5000], Step [2/2], Loss: 0.1519\n",
      "Epoch [4200/5000], Step [2/2], Loss: 0.1251\n",
      "Epoch [4300/5000], Step [2/2], Loss: 0.1357\n",
      "Epoch [4400/5000], Step [2/2], Loss: 0.1551\n",
      "Epoch [4500/5000], Step [2/2], Loss: 0.1491\n",
      "Epoch [4600/5000], Step [2/2], Loss: 0.1302\n",
      "Epoch [4700/5000], Step [2/2], Loss: 0.1199\n",
      "Epoch [4800/5000], Step [2/2], Loss: 0.1020\n",
      "Epoch [4900/5000], Step [2/2], Loss: 0.1092\n",
      "Epoch [5000/5000], Step [2/2], Loss: 0.1220\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 1000 iteration 0\n",
      "Started at 07/28/2019, 22:59:07\n",
      "The time is 07/28/2019, 22:59:43\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [2/2], Loss: 0.2831\n",
      "Epoch [200/5000], Step [2/2], Loss: 0.2665\n",
      "Epoch [300/5000], Step [2/2], Loss: 0.2593\n",
      "Epoch [400/5000], Step [2/2], Loss: 0.2391\n",
      "Epoch [500/5000], Step [2/2], Loss: 0.2351\n",
      "Epoch [600/5000], Step [2/2], Loss: 0.2129\n",
      "Epoch [700/5000], Step [2/2], Loss: 0.2170\n",
      "Epoch [800/5000], Step [2/2], Loss: 0.2204\n",
      "Epoch [900/5000], Step [2/2], Loss: 0.1646\n",
      "Epoch [1000/5000], Step [2/2], Loss: 0.1946\n",
      "Epoch [1100/5000], Step [2/2], Loss: 0.2401\n",
      "Epoch [1200/5000], Step [2/2], Loss: 0.2162\n",
      "Epoch [1300/5000], Step [2/2], Loss: 0.2362\n",
      "Epoch [1400/5000], Step [2/2], Loss: 0.2384\n",
      "Epoch [1500/5000], Step [2/2], Loss: 0.1916\n",
      "Epoch [1600/5000], Step [2/2], Loss: 0.1664\n",
      "Epoch [1700/5000], Step [2/2], Loss: 0.2006\n",
      "Epoch [1800/5000], Step [2/2], Loss: 0.2248\n",
      "Epoch [1900/5000], Step [2/2], Loss: 0.1960\n",
      "Epoch [2000/5000], Step [2/2], Loss: 0.1687\n",
      "Epoch [2100/5000], Step [2/2], Loss: 0.1849\n",
      "Epoch [2200/5000], Step [2/2], Loss: 0.1674\n",
      "Epoch [2300/5000], Step [2/2], Loss: 0.1768\n",
      "Epoch [2400/5000], Step [2/2], Loss: 0.1792\n",
      "Epoch [2500/5000], Step [2/2], Loss: 0.1594\n",
      "Epoch [2600/5000], Step [2/2], Loss: 0.1888\n",
      "Epoch [2700/5000], Step [2/2], Loss: 0.1388\n",
      "Epoch [2800/5000], Step [2/2], Loss: 0.1422\n",
      "Epoch [2900/5000], Step [2/2], Loss: 0.1431\n",
      "Epoch [3000/5000], Step [2/2], Loss: 0.1708\n",
      "Epoch [3100/5000], Step [2/2], Loss: 0.1517\n",
      "Epoch [3200/5000], Step [2/2], Loss: 0.2123\n",
      "Epoch [3300/5000], Step [2/2], Loss: 0.1417\n",
      "Epoch [3400/5000], Step [2/2], Loss: 0.1719\n",
      "Epoch [3500/5000], Step [2/2], Loss: 0.1303\n",
      "Epoch [3600/5000], Step [2/2], Loss: 0.1690\n",
      "Epoch [3700/5000], Step [2/2], Loss: 0.1122\n",
      "Epoch [3800/5000], Step [2/2], Loss: 0.1223\n",
      "Epoch [3900/5000], Step [2/2], Loss: 0.1414\n",
      "Epoch [4000/5000], Step [2/2], Loss: 0.1151\n",
      "Epoch [4100/5000], Step [2/2], Loss: 0.1290\n",
      "Epoch [4200/5000], Step [2/2], Loss: 0.1495\n",
      "Epoch [4300/5000], Step [2/2], Loss: 0.1149\n",
      "Epoch [4400/5000], Step [2/2], Loss: 0.1517\n",
      "Epoch [4500/5000], Step [2/2], Loss: 0.1478\n",
      "Epoch [4600/5000], Step [2/2], Loss: 0.1168\n",
      "Epoch [4700/5000], Step [2/2], Loss: 0.1530\n",
      "Epoch [4800/5000], Step [2/2], Loss: 0.1165\n",
      "Epoch [4900/5000], Step [2/2], Loss: 0.1320\n",
      "Epoch [5000/5000], Step [2/2], Loss: 0.1015\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 1000 iteration 1\n",
      "Started at 07/28/2019, 22:59:43\n",
      "The time is 07/28/2019, 23:00:16\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [2/2], Loss: 0.3934\n",
      "Epoch [200/5000], Step [2/2], Loss: 0.2510\n",
      "Epoch [300/5000], Step [2/2], Loss: 0.2425\n",
      "Epoch [400/5000], Step [2/2], Loss: 0.2070\n",
      "Epoch [500/5000], Step [2/2], Loss: 0.2422\n",
      "Epoch [600/5000], Step [2/2], Loss: 0.2264\n",
      "Epoch [700/5000], Step [2/2], Loss: 0.2170\n",
      "Epoch [800/5000], Step [2/2], Loss: 0.2195\n",
      "Epoch [900/5000], Step [2/2], Loss: 0.2422\n",
      "Epoch [1000/5000], Step [2/2], Loss: 0.2091\n",
      "Epoch [1100/5000], Step [2/2], Loss: 0.1839\n",
      "Epoch [1200/5000], Step [2/2], Loss: 0.1796\n",
      "Epoch [1300/5000], Step [2/2], Loss: 0.1878\n",
      "Epoch [1400/5000], Step [2/2], Loss: 0.1520\n",
      "Epoch [1500/5000], Step [2/2], Loss: 0.2246\n",
      "Epoch [1600/5000], Step [2/2], Loss: 0.2085\n",
      "Epoch [1700/5000], Step [2/2], Loss: 0.2081\n",
      "Epoch [1800/5000], Step [2/2], Loss: 0.2037\n",
      "Epoch [1900/5000], Step [2/2], Loss: 0.1986\n",
      "Epoch [2000/5000], Step [2/2], Loss: 0.2082\n",
      "Epoch [2100/5000], Step [2/2], Loss: 0.1888\n",
      "Epoch [2200/5000], Step [2/2], Loss: 0.1749\n",
      "Epoch [2300/5000], Step [2/2], Loss: 0.1742\n",
      "Epoch [2400/5000], Step [2/2], Loss: 0.1764\n",
      "Epoch [2500/5000], Step [2/2], Loss: 0.1941\n",
      "Epoch [2600/5000], Step [2/2], Loss: 0.1709\n",
      "Epoch [2700/5000], Step [2/2], Loss: 0.1428\n",
      "Epoch [2800/5000], Step [2/2], Loss: 0.1655\n",
      "Epoch [2900/5000], Step [2/2], Loss: 0.1565\n",
      "Epoch [3000/5000], Step [2/2], Loss: 0.1878\n",
      "Epoch [3100/5000], Step [2/2], Loss: 0.1483\n",
      "Epoch [3200/5000], Step [2/2], Loss: 0.1241\n",
      "Epoch [3300/5000], Step [2/2], Loss: 0.1667\n",
      "Epoch [3400/5000], Step [2/2], Loss: 0.1474\n",
      "Epoch [3500/5000], Step [2/2], Loss: 0.1402\n",
      "Epoch [3600/5000], Step [2/2], Loss: 0.1592\n",
      "Epoch [3700/5000], Step [2/2], Loss: 0.1646\n",
      "Epoch [3800/5000], Step [2/2], Loss: 0.1074\n",
      "Epoch [3900/5000], Step [2/2], Loss: 0.1551\n",
      "Epoch [4000/5000], Step [2/2], Loss: 0.1451\n",
      "Epoch [4100/5000], Step [2/2], Loss: 0.1002\n",
      "Epoch [4200/5000], Step [2/2], Loss: 0.1322\n",
      "Epoch [4300/5000], Step [2/2], Loss: 0.1395\n",
      "Epoch [4400/5000], Step [2/2], Loss: 0.1605\n",
      "Epoch [4500/5000], Step [2/2], Loss: 0.1404\n",
      "Epoch [4600/5000], Step [2/2], Loss: 0.1223\n",
      "Epoch [4700/5000], Step [2/2], Loss: 0.1061\n",
      "Epoch [4800/5000], Step [2/2], Loss: 0.1396\n",
      "Epoch [4900/5000], Step [2/2], Loss: 0.0956\n",
      "Epoch [5000/5000], Step [2/2], Loss: 0.1138\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 1000 iteration 2\n",
      "Started at 07/28/2019, 23:00:16\n",
      "The time is 07/28/2019, 23:00:50\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [2/2], Loss: 0.3536\n",
      "Epoch [200/5000], Step [2/2], Loss: 0.2294\n",
      "Epoch [300/5000], Step [2/2], Loss: 0.2113\n",
      "Epoch [400/5000], Step [2/2], Loss: 0.2143\n",
      "Epoch [500/5000], Step [2/2], Loss: 0.2399\n",
      "Epoch [600/5000], Step [2/2], Loss: 0.2078\n",
      "Epoch [700/5000], Step [2/2], Loss: 0.1940\n",
      "Epoch [800/5000], Step [2/2], Loss: 0.2346\n",
      "Epoch [900/5000], Step [2/2], Loss: 0.2543\n",
      "Epoch [1000/5000], Step [2/2], Loss: 0.1663\n",
      "Epoch [1100/5000], Step [2/2], Loss: 0.1699\n",
      "Epoch [1200/5000], Step [2/2], Loss: 0.2381\n",
      "Epoch [1300/5000], Step [2/2], Loss: 0.2190\n",
      "Epoch [1400/5000], Step [2/2], Loss: 0.2001\n",
      "Epoch [1500/5000], Step [2/2], Loss: 0.2337\n",
      "Epoch [1600/5000], Step [2/2], Loss: 0.2489\n",
      "Epoch [1700/5000], Step [2/2], Loss: 0.2035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1800/5000], Step [2/2], Loss: 0.2433\n",
      "Epoch [1900/5000], Step [2/2], Loss: 0.1630\n",
      "Epoch [2000/5000], Step [2/2], Loss: 0.2294\n",
      "Epoch [2100/5000], Step [2/2], Loss: 0.1819\n",
      "Epoch [2200/5000], Step [2/2], Loss: 0.1690\n",
      "Epoch [2300/5000], Step [2/2], Loss: 0.2069\n",
      "Epoch [2400/5000], Step [2/2], Loss: 0.1946\n",
      "Epoch [2500/5000], Step [2/2], Loss: 0.2003\n",
      "Epoch [2600/5000], Step [2/2], Loss: 0.1849\n",
      "Epoch [2700/5000], Step [2/2], Loss: 0.1918\n",
      "Epoch [2800/5000], Step [2/2], Loss: 0.1442\n",
      "Epoch [2900/5000], Step [2/2], Loss: 0.1799\n",
      "Epoch [3000/5000], Step [2/2], Loss: 0.1734\n",
      "Epoch [3100/5000], Step [2/2], Loss: 0.1694\n",
      "Epoch [3200/5000], Step [2/2], Loss: 0.1577\n",
      "Epoch [3300/5000], Step [2/2], Loss: 0.1605\n",
      "Epoch [3400/5000], Step [2/2], Loss: 0.1940\n",
      "Epoch [3500/5000], Step [2/2], Loss: 0.1180\n",
      "Epoch [3600/5000], Step [2/2], Loss: 0.1736\n",
      "Epoch [3700/5000], Step [2/2], Loss: 0.1382\n",
      "Epoch [3800/5000], Step [2/2], Loss: 0.1179\n",
      "Epoch [3900/5000], Step [2/2], Loss: 0.1428\n",
      "Epoch [4000/5000], Step [2/2], Loss: 0.1544\n",
      "Epoch [4100/5000], Step [2/2], Loss: 0.1484\n",
      "Epoch [4200/5000], Step [2/2], Loss: 0.1522\n",
      "Epoch [4300/5000], Step [2/2], Loss: 0.1350\n",
      "Epoch [4400/5000], Step [2/2], Loss: 0.1895\n",
      "Epoch [4500/5000], Step [2/2], Loss: 0.1657\n",
      "Epoch [4600/5000], Step [2/2], Loss: 0.1141\n",
      "Epoch [4700/5000], Step [2/2], Loss: 0.1341\n",
      "Epoch [4800/5000], Step [2/2], Loss: 0.1257\n",
      "Epoch [4900/5000], Step [2/2], Loss: 0.1403\n",
      "Epoch [5000/5000], Step [2/2], Loss: 0.1533\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 1000 iteration 3\n",
      "Started at 07/28/2019, 23:00:50\n",
      "The time is 07/28/2019, 23:01:23\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [2/2], Loss: 0.3234\n",
      "Epoch [200/5000], Step [2/2], Loss: 0.2845\n",
      "Epoch [300/5000], Step [2/2], Loss: 0.2839\n",
      "Epoch [400/5000], Step [2/2], Loss: 0.2447\n",
      "Epoch [500/5000], Step [2/2], Loss: 0.2513\n",
      "Epoch [600/5000], Step [2/2], Loss: 0.1775\n",
      "Epoch [700/5000], Step [2/2], Loss: 0.1943\n",
      "Epoch [800/5000], Step [2/2], Loss: 0.1947\n",
      "Epoch [900/5000], Step [2/2], Loss: 0.2152\n",
      "Epoch [1000/5000], Step [2/2], Loss: 0.2231\n",
      "Epoch [1100/5000], Step [2/2], Loss: 0.1896\n",
      "Epoch [1200/5000], Step [2/2], Loss: 0.1904\n",
      "Epoch [1300/5000], Step [2/2], Loss: 0.1900\n",
      "Epoch [1400/5000], Step [2/2], Loss: 0.1727\n",
      "Epoch [1500/5000], Step [2/2], Loss: 0.1656\n",
      "Epoch [1600/5000], Step [2/2], Loss: 0.1643\n",
      "Epoch [1700/5000], Step [2/2], Loss: 0.1609\n",
      "Epoch [1800/5000], Step [2/2], Loss: 0.1405\n",
      "Epoch [1900/5000], Step [2/2], Loss: 0.1732\n",
      "Epoch [2000/5000], Step [2/2], Loss: 0.1864\n",
      "Epoch [2100/5000], Step [2/2], Loss: 0.1891\n",
      "Epoch [2200/5000], Step [2/2], Loss: 0.1577\n",
      "Epoch [2300/5000], Step [2/2], Loss: 0.1724\n",
      "Epoch [2400/5000], Step [2/2], Loss: 0.1638\n",
      "Epoch [2500/5000], Step [2/2], Loss: 0.1637\n",
      "Epoch [2600/5000], Step [2/2], Loss: 0.1472\n",
      "Epoch [2700/5000], Step [2/2], Loss: 0.1194\n",
      "Epoch [2800/5000], Step [2/2], Loss: 0.1401\n",
      "Epoch [2900/5000], Step [2/2], Loss: 0.1541\n",
      "Epoch [3000/5000], Step [2/2], Loss: 0.1239\n",
      "Epoch [3100/5000], Step [2/2], Loss: 0.1301\n",
      "Epoch [3200/5000], Step [2/2], Loss: 0.1451\n",
      "Epoch [3300/5000], Step [2/2], Loss: 0.1079\n",
      "Epoch [3400/5000], Step [2/2], Loss: 0.1333\n",
      "Epoch [3500/5000], Step [2/2], Loss: 0.1517\n",
      "Epoch [3600/5000], Step [2/2], Loss: 0.1197\n",
      "Epoch [3700/5000], Step [2/2], Loss: 0.1170\n",
      "Epoch [3800/5000], Step [2/2], Loss: 0.1362\n",
      "Epoch [3900/5000], Step [2/2], Loss: 0.0916\n",
      "Epoch [4000/5000], Step [2/2], Loss: 0.0952\n",
      "Epoch [4100/5000], Step [2/2], Loss: 0.1066\n",
      "Epoch [4200/5000], Step [2/2], Loss: 0.1050\n",
      "Epoch [4300/5000], Step [2/2], Loss: 0.0913\n",
      "Epoch [4400/5000], Step [2/2], Loss: 0.1275\n",
      "Epoch [4500/5000], Step [2/2], Loss: 0.1119\n",
      "Epoch [4600/5000], Step [2/2], Loss: 0.0853\n",
      "Epoch [4700/5000], Step [2/2], Loss: 0.0987\n",
      "Epoch [4800/5000], Step [2/2], Loss: 0.1167\n",
      "Epoch [4900/5000], Step [2/2], Loss: 0.0740\n",
      "Epoch [5000/5000], Step [2/2], Loss: 0.0872\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 1000 iteration 4\n",
      "Started at 07/28/2019, 23:01:23\n",
      "The time is 07/28/2019, 23:01:56\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [4/4], Loss: 0.2295\n",
      "Epoch [200/5000], Step [4/4], Loss: 0.1994\n",
      "Epoch [300/5000], Step [4/4], Loss: 0.1905\n",
      "Epoch [400/5000], Step [4/4], Loss: 0.1867\n",
      "Epoch [500/5000], Step [4/4], Loss: 0.1458\n",
      "Epoch [600/5000], Step [4/4], Loss: 0.1917\n",
      "Epoch [700/5000], Step [4/4], Loss: 0.1898\n",
      "Epoch [800/5000], Step [4/4], Loss: 0.2031\n",
      "Epoch [900/5000], Step [4/4], Loss: 0.1830\n",
      "Epoch [1000/5000], Step [4/4], Loss: 0.2316\n",
      "Epoch [1100/5000], Step [4/4], Loss: 0.2278\n",
      "Epoch [1200/5000], Step [4/4], Loss: 0.1896\n",
      "Epoch [1300/5000], Step [4/4], Loss: 0.1813\n",
      "Epoch [1400/5000], Step [4/4], Loss: 0.2050\n",
      "Epoch [1500/5000], Step [4/4], Loss: 0.1919\n",
      "Epoch [1600/5000], Step [4/4], Loss: 0.1979\n",
      "Epoch [1700/5000], Step [4/4], Loss: 0.1837\n",
      "Epoch [1800/5000], Step [4/4], Loss: 0.2042\n",
      "Epoch [1900/5000], Step [4/4], Loss: 0.1712\n",
      "Epoch [2000/5000], Step [4/4], Loss: 0.1862\n",
      "Epoch [2100/5000], Step [4/4], Loss: 0.1651\n",
      "Epoch [2200/5000], Step [4/4], Loss: 0.2077\n",
      "Epoch [2300/5000], Step [4/4], Loss: 0.1726\n",
      "Epoch [2400/5000], Step [4/4], Loss: 0.1548\n",
      "Epoch [2500/5000], Step [4/4], Loss: 0.1749\n",
      "Epoch [2600/5000], Step [4/4], Loss: 0.1861\n",
      "Epoch [2700/5000], Step [4/4], Loss: 0.2052\n",
      "Epoch [2800/5000], Step [4/4], Loss: 0.1766\n",
      "Epoch [2900/5000], Step [4/4], Loss: 0.1674\n",
      "Epoch [3000/5000], Step [4/4], Loss: 0.1775\n",
      "Epoch [3100/5000], Step [4/4], Loss: 0.1959\n",
      "Epoch [3200/5000], Step [4/4], Loss: 0.1753\n",
      "Epoch [3300/5000], Step [4/4], Loss: 0.1804\n",
      "Epoch [3400/5000], Step [4/4], Loss: 0.1599\n",
      "Epoch [3500/5000], Step [4/4], Loss: 0.1202\n",
      "Epoch [3600/5000], Step [4/4], Loss: 0.2072\n",
      "Epoch [3700/5000], Step [4/4], Loss: 0.1842\n",
      "Epoch [3800/5000], Step [4/4], Loss: 0.1778\n",
      "Epoch [3900/5000], Step [4/4], Loss: 0.1671\n",
      "Epoch [4000/5000], Step [4/4], Loss: 0.1890\n",
      "Epoch [4100/5000], Step [4/4], Loss: 0.1675\n",
      "Epoch [4200/5000], Step [4/4], Loss: 0.2048\n",
      "Epoch [4300/5000], Step [4/4], Loss: 0.1716\n",
      "Epoch [4400/5000], Step [4/4], Loss: 0.1463\n",
      "Epoch [4500/5000], Step [4/4], Loss: 0.1578\n",
      "Epoch [4600/5000], Step [4/4], Loss: 0.1633\n",
      "Epoch [4700/5000], Step [4/4], Loss: 0.2138\n",
      "Epoch [4800/5000], Step [4/4], Loss: 0.1865\n",
      "Epoch [4900/5000], Step [4/4], Loss: 0.1271\n",
      "Epoch [5000/5000], Step [4/4], Loss: 0.1834\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 2500 iteration 0\n",
      "Started at 07/28/2019, 23:01:56\n",
      "The time is 07/28/2019, 23:03:12\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [4/4], Loss: 0.1921\n",
      "Epoch [200/5000], Step [4/4], Loss: 0.1841\n",
      "Epoch [300/5000], Step [4/4], Loss: 0.1634\n",
      "Epoch [400/5000], Step [4/4], Loss: 0.2360\n",
      "Epoch [500/5000], Step [4/4], Loss: 0.2121\n",
      "Epoch [600/5000], Step [4/4], Loss: 0.2019\n",
      "Epoch [700/5000], Step [4/4], Loss: 0.1643\n",
      "Epoch [800/5000], Step [4/4], Loss: 0.2363\n",
      "Epoch [900/5000], Step [4/4], Loss: 0.1760\n",
      "Epoch [1000/5000], Step [4/4], Loss: 0.1421\n",
      "Epoch [1100/5000], Step [4/4], Loss: 0.1822\n",
      "Epoch [1200/5000], Step [4/4], Loss: 0.1730\n",
      "Epoch [1300/5000], Step [4/4], Loss: 0.2065\n",
      "Epoch [1400/5000], Step [4/4], Loss: 0.1840\n",
      "Epoch [1500/5000], Step [4/4], Loss: 0.1676\n",
      "Epoch [1600/5000], Step [4/4], Loss: 0.1730\n",
      "Epoch [1700/5000], Step [4/4], Loss: 0.1691\n",
      "Epoch [1800/5000], Step [4/4], Loss: 0.1867\n",
      "Epoch [1900/5000], Step [4/4], Loss: 0.1729\n",
      "Epoch [2000/5000], Step [4/4], Loss: 0.1871\n",
      "Epoch [2100/5000], Step [4/4], Loss: 0.1777\n",
      "Epoch [2200/5000], Step [4/4], Loss: 0.1646\n",
      "Epoch [2300/5000], Step [4/4], Loss: 0.1699\n",
      "Epoch [2400/5000], Step [4/4], Loss: 0.1645\n",
      "Epoch [2500/5000], Step [4/4], Loss: 0.1850\n",
      "Epoch [2600/5000], Step [4/4], Loss: 0.1447\n",
      "Epoch [2700/5000], Step [4/4], Loss: 0.1644\n",
      "Epoch [2800/5000], Step [4/4], Loss: 0.1398\n",
      "Epoch [2900/5000], Step [4/4], Loss: 0.1292\n",
      "Epoch [3000/5000], Step [4/4], Loss: 0.1724\n",
      "Epoch [3100/5000], Step [4/4], Loss: 0.2074\n",
      "Epoch [3200/5000], Step [4/4], Loss: 0.1336\n",
      "Epoch [3300/5000], Step [4/4], Loss: 0.1346\n",
      "Epoch [3400/5000], Step [4/4], Loss: 0.1543\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3500/5000], Step [4/4], Loss: 0.1584\n",
      "Epoch [3600/5000], Step [4/4], Loss: 0.1652\n",
      "Epoch [3700/5000], Step [4/4], Loss: 0.1494\n",
      "Epoch [3800/5000], Step [4/4], Loss: 0.1712\n",
      "Epoch [3900/5000], Step [4/4], Loss: 0.1777\n",
      "Epoch [4000/5000], Step [4/4], Loss: 0.1356\n",
      "Epoch [4100/5000], Step [4/4], Loss: 0.1538\n",
      "Epoch [4200/5000], Step [4/4], Loss: 0.1651\n",
      "Epoch [4300/5000], Step [4/4], Loss: 0.1406\n",
      "Epoch [4400/5000], Step [4/4], Loss: 0.1755\n",
      "Epoch [4500/5000], Step [4/4], Loss: 0.1506\n",
      "Epoch [4600/5000], Step [4/4], Loss: 0.1372\n",
      "Epoch [4700/5000], Step [4/4], Loss: 0.1392\n",
      "Epoch [4800/5000], Step [4/4], Loss: 0.1619\n",
      "Epoch [4900/5000], Step [4/4], Loss: 0.1411\n",
      "Epoch [5000/5000], Step [4/4], Loss: 0.1527\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 2500 iteration 1\n",
      "Started at 07/28/2019, 23:03:12\n",
      "The time is 07/28/2019, 23:04:28\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [4/4], Loss: 0.2320\n",
      "Epoch [200/5000], Step [4/4], Loss: 0.2475\n",
      "Epoch [300/5000], Step [4/4], Loss: 0.1964\n",
      "Epoch [400/5000], Step [4/4], Loss: 0.2073\n",
      "Epoch [500/5000], Step [4/4], Loss: 0.2131\n",
      "Epoch [600/5000], Step [4/4], Loss: 0.2022\n",
      "Epoch [700/5000], Step [4/4], Loss: 0.1839\n",
      "Epoch [800/5000], Step [4/4], Loss: 0.1718\n",
      "Epoch [900/5000], Step [4/4], Loss: 0.2006\n",
      "Epoch [1000/5000], Step [4/4], Loss: 0.2225\n",
      "Epoch [1100/5000], Step [4/4], Loss: 0.2170\n",
      "Epoch [1200/5000], Step [4/4], Loss: 0.2042\n",
      "Epoch [1300/5000], Step [4/4], Loss: 0.1787\n",
      "Epoch [1400/5000], Step [4/4], Loss: 0.1841\n",
      "Epoch [1500/5000], Step [4/4], Loss: 0.2157\n",
      "Epoch [1600/5000], Step [4/4], Loss: 0.1653\n",
      "Epoch [1700/5000], Step [4/4], Loss: 0.2156\n",
      "Epoch [1800/5000], Step [4/4], Loss: 0.1727\n",
      "Epoch [1900/5000], Step [4/4], Loss: 0.2045\n",
      "Epoch [2000/5000], Step [4/4], Loss: 0.1716\n",
      "Epoch [2100/5000], Step [4/4], Loss: 0.1865\n",
      "Epoch [2200/5000], Step [4/4], Loss: 0.1902\n",
      "Epoch [2300/5000], Step [4/4], Loss: 0.1381\n",
      "Epoch [2400/5000], Step [4/4], Loss: 0.2033\n",
      "Epoch [2500/5000], Step [4/4], Loss: 0.1556\n",
      "Epoch [2600/5000], Step [4/4], Loss: 0.1742\n",
      "Epoch [2700/5000], Step [4/4], Loss: 0.2005\n",
      "Epoch [2800/5000], Step [4/4], Loss: 0.1789\n",
      "Epoch [2900/5000], Step [4/4], Loss: 0.1790\n",
      "Epoch [3000/5000], Step [4/4], Loss: 0.1854\n",
      "Epoch [3100/5000], Step [4/4], Loss: 0.1852\n",
      "Epoch [3200/5000], Step [4/4], Loss: 0.1370\n",
      "Epoch [3300/5000], Step [4/4], Loss: 0.1601\n",
      "Epoch [3400/5000], Step [4/4], Loss: 0.1778\n",
      "Epoch [3500/5000], Step [4/4], Loss: 0.1437\n",
      "Epoch [3600/5000], Step [4/4], Loss: 0.1487\n",
      "Epoch [3700/5000], Step [4/4], Loss: 0.1800\n",
      "Epoch [3800/5000], Step [4/4], Loss: 0.1548\n",
      "Epoch [3900/5000], Step [4/4], Loss: 0.1517\n",
      "Epoch [4000/5000], Step [4/4], Loss: 0.1817\n",
      "Epoch [4100/5000], Step [4/4], Loss: 0.1599\n",
      "Epoch [4200/5000], Step [4/4], Loss: 0.1623\n",
      "Epoch [4300/5000], Step [4/4], Loss: 0.1322\n",
      "Epoch [4400/5000], Step [4/4], Loss: 0.1646\n",
      "Epoch [4500/5000], Step [4/4], Loss: 0.1221\n",
      "Epoch [4600/5000], Step [4/4], Loss: 0.1532\n",
      "Epoch [4700/5000], Step [4/4], Loss: 0.1384\n",
      "Epoch [4800/5000], Step [4/4], Loss: 0.1677\n",
      "Epoch [4900/5000], Step [4/4], Loss: 0.1649\n",
      "Epoch [5000/5000], Step [4/4], Loss: 0.1525\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 2500 iteration 2\n",
      "Started at 07/28/2019, 23:04:28\n",
      "The time is 07/28/2019, 23:05:44\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [4/4], Loss: 0.2166\n",
      "Epoch [200/5000], Step [4/4], Loss: 0.1753\n",
      "Epoch [300/5000], Step [4/4], Loss: 0.2062\n",
      "Epoch [400/5000], Step [4/4], Loss: 0.1747\n",
      "Epoch [500/5000], Step [4/4], Loss: 0.2036\n",
      "Epoch [600/5000], Step [4/4], Loss: 0.2332\n",
      "Epoch [700/5000], Step [4/4], Loss: 0.1843\n",
      "Epoch [800/5000], Step [4/4], Loss: 0.2152\n",
      "Epoch [900/5000], Step [4/4], Loss: 0.2091\n",
      "Epoch [1000/5000], Step [4/4], Loss: 0.1623\n",
      "Epoch [1100/5000], Step [4/4], Loss: 0.2076\n",
      "Epoch [1200/5000], Step [4/4], Loss: 0.1594\n",
      "Epoch [1300/5000], Step [4/4], Loss: 0.1850\n",
      "Epoch [1400/5000], Step [4/4], Loss: 0.1742\n",
      "Epoch [1500/5000], Step [4/4], Loss: 0.1965\n",
      "Epoch [1600/5000], Step [4/4], Loss: 0.2029\n",
      "Epoch [1700/5000], Step [4/4], Loss: 0.1906\n",
      "Epoch [1800/5000], Step [4/4], Loss: 0.1679\n",
      "Epoch [1900/5000], Step [4/4], Loss: 0.1539\n",
      "Epoch [2000/5000], Step [4/4], Loss: 0.1458\n",
      "Epoch [2100/5000], Step [4/4], Loss: 0.1657\n",
      "Epoch [2200/5000], Step [4/4], Loss: 0.1926\n",
      "Epoch [2300/5000], Step [4/4], Loss: 0.1579\n",
      "Epoch [2400/5000], Step [4/4], Loss: 0.2022\n",
      "Epoch [2500/5000], Step [4/4], Loss: 0.2080\n",
      "Epoch [2600/5000], Step [4/4], Loss: 0.2115\n",
      "Epoch [2700/5000], Step [4/4], Loss: 0.1660\n",
      "Epoch [2800/5000], Step [4/4], Loss: 0.1849\n",
      "Epoch [2900/5000], Step [4/4], Loss: 0.1734\n",
      "Epoch [3000/5000], Step [4/4], Loss: 0.1434\n",
      "Epoch [3100/5000], Step [4/4], Loss: 0.1573\n",
      "Epoch [3200/5000], Step [4/4], Loss: 0.1527\n",
      "Epoch [3300/5000], Step [4/4], Loss: 0.1986\n",
      "Epoch [3400/5000], Step [4/4], Loss: 0.1394\n",
      "Epoch [3500/5000], Step [4/4], Loss: 0.1648\n",
      "Epoch [3600/5000], Step [4/4], Loss: 0.1555\n",
      "Epoch [3700/5000], Step [4/4], Loss: 0.1605\n",
      "Epoch [3800/5000], Step [4/4], Loss: 0.1581\n",
      "Epoch [3900/5000], Step [4/4], Loss: 0.1559\n",
      "Epoch [4000/5000], Step [4/4], Loss: 0.1461\n",
      "Epoch [4100/5000], Step [4/4], Loss: 0.1376\n",
      "Epoch [4200/5000], Step [4/4], Loss: 0.1544\n",
      "Epoch [4300/5000], Step [4/4], Loss: 0.1526\n",
      "Epoch [4400/5000], Step [4/4], Loss: 0.1387\n",
      "Epoch [4500/5000], Step [4/4], Loss: 0.1900\n",
      "Epoch [4600/5000], Step [4/4], Loss: 0.1916\n",
      "Epoch [4700/5000], Step [4/4], Loss: 0.1321\n",
      "Epoch [4800/5000], Step [4/4], Loss: 0.1725\n",
      "Epoch [4900/5000], Step [4/4], Loss: 0.1524\n",
      "Epoch [5000/5000], Step [4/4], Loss: 0.1398\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 2500 iteration 3\n",
      "Started at 07/28/2019, 23:05:44\n",
      "The time is 07/28/2019, 23:07:00\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [4/4], Loss: 0.2731\n",
      "Epoch [200/5000], Step [4/4], Loss: 0.2253\n",
      "Epoch [300/5000], Step [4/4], Loss: 0.2230\n",
      "Epoch [400/5000], Step [4/4], Loss: 0.2018\n",
      "Epoch [500/5000], Step [4/4], Loss: 0.1909\n",
      "Epoch [600/5000], Step [4/4], Loss: 0.2015\n",
      "Epoch [700/5000], Step [4/4], Loss: 0.2036\n",
      "Epoch [800/5000], Step [4/4], Loss: 0.1948\n",
      "Epoch [900/5000], Step [4/4], Loss: 0.1636\n",
      "Epoch [1000/5000], Step [4/4], Loss: 0.1768\n",
      "Epoch [1100/5000], Step [4/4], Loss: 0.1710\n",
      "Epoch [1200/5000], Step [4/4], Loss: 0.1768\n",
      "Epoch [1300/5000], Step [4/4], Loss: 0.1463\n",
      "Epoch [1400/5000], Step [4/4], Loss: 0.1767\n",
      "Epoch [1500/5000], Step [4/4], Loss: 0.1580\n",
      "Epoch [1600/5000], Step [4/4], Loss: 0.1937\n",
      "Epoch [1700/5000], Step [4/4], Loss: 0.1880\n",
      "Epoch [1800/5000], Step [4/4], Loss: 0.1652\n",
      "Epoch [1900/5000], Step [4/4], Loss: 0.1793\n",
      "Epoch [2000/5000], Step [4/4], Loss: 0.1733\n",
      "Epoch [2100/5000], Step [4/4], Loss: 0.1770\n",
      "Epoch [2200/5000], Step [4/4], Loss: 0.1828\n",
      "Epoch [2300/5000], Step [4/4], Loss: 0.1602\n",
      "Epoch [2400/5000], Step [4/4], Loss: 0.1673\n",
      "Epoch [2500/5000], Step [4/4], Loss: 0.1646\n",
      "Epoch [2600/5000], Step [4/4], Loss: 0.1682\n",
      "Epoch [2700/5000], Step [4/4], Loss: 0.1292\n",
      "Epoch [2800/5000], Step [4/4], Loss: 0.1772\n",
      "Epoch [2900/5000], Step [4/4], Loss: 0.1569\n",
      "Epoch [3000/5000], Step [4/4], Loss: 0.1961\n",
      "Epoch [3100/5000], Step [4/4], Loss: 0.1762\n",
      "Epoch [3200/5000], Step [4/4], Loss: 0.1511\n",
      "Epoch [3300/5000], Step [4/4], Loss: 0.1505\n",
      "Epoch [3400/5000], Step [4/4], Loss: 0.1731\n",
      "Epoch [3500/5000], Step [4/4], Loss: 0.1713\n",
      "Epoch [3600/5000], Step [4/4], Loss: 0.1409\n",
      "Epoch [3700/5000], Step [4/4], Loss: 0.1555\n",
      "Epoch [3800/5000], Step [4/4], Loss: 0.1384\n",
      "Epoch [3900/5000], Step [4/4], Loss: 0.2080\n",
      "Epoch [4000/5000], Step [4/4], Loss: 0.1303\n",
      "Epoch [4100/5000], Step [4/4], Loss: 0.1671\n",
      "Epoch [4200/5000], Step [4/4], Loss: 0.1571\n",
      "Epoch [4300/5000], Step [4/4], Loss: 0.1578\n",
      "Epoch [4400/5000], Step [4/4], Loss: 0.1766\n",
      "Epoch [4500/5000], Step [4/4], Loss: 0.1830\n",
      "Epoch [4600/5000], Step [4/4], Loss: 0.1690\n",
      "Epoch [4700/5000], Step [4/4], Loss: 0.1394\n",
      "Epoch [4800/5000], Step [4/4], Loss: 0.1414\n",
      "Epoch [4900/5000], Step [4/4], Loss: 0.1521\n",
      "Epoch [5000/5000], Step [4/4], Loss: 0.1301\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 2500 iteration 4\n",
      "Started at 07/28/2019, 23:07:00\n",
      "The time is 07/28/2019, 23:08:16\n",
      "################################################\n",
      "################################################\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [100/5000], Step [8/8], Loss: 0.1801\n",
      "Epoch [200/5000], Step [8/8], Loss: 0.2395\n",
      "Epoch [300/5000], Step [8/8], Loss: 0.2066\n",
      "Epoch [400/5000], Step [8/8], Loss: 0.2262\n",
      "Epoch [500/5000], Step [8/8], Loss: 0.2038\n",
      "Epoch [600/5000], Step [8/8], Loss: 0.2173\n",
      "Epoch [700/5000], Step [8/8], Loss: 0.2462\n",
      "Epoch [800/5000], Step [8/8], Loss: 0.2084\n",
      "Epoch [900/5000], Step [8/8], Loss: 0.1709\n",
      "Epoch [1000/5000], Step [8/8], Loss: 0.1504\n",
      "Epoch [1100/5000], Step [8/8], Loss: 0.2061\n",
      "Epoch [1200/5000], Step [8/8], Loss: 0.1928\n",
      "Epoch [1300/5000], Step [8/8], Loss: 0.1788\n",
      "Epoch [1400/5000], Step [8/8], Loss: 0.2363\n",
      "Epoch [1500/5000], Step [8/8], Loss: 0.2065\n",
      "Epoch [1600/5000], Step [8/8], Loss: 0.1988\n",
      "Epoch [1700/5000], Step [8/8], Loss: 0.1906\n",
      "Epoch [1800/5000], Step [8/8], Loss: 0.2431\n",
      "Epoch [1900/5000], Step [8/8], Loss: 0.1669\n",
      "Epoch [2000/5000], Step [8/8], Loss: 0.2048\n",
      "Epoch [2100/5000], Step [8/8], Loss: 0.1919\n",
      "Epoch [2200/5000], Step [8/8], Loss: 0.1960\n",
      "Epoch [2300/5000], Step [8/8], Loss: 0.1906\n",
      "Epoch [2400/5000], Step [8/8], Loss: 0.1948\n",
      "Epoch [2500/5000], Step [8/8], Loss: 0.2032\n",
      "Epoch [2600/5000], Step [8/8], Loss: 0.2287\n",
      "Epoch [2700/5000], Step [8/8], Loss: 0.2032\n",
      "Epoch [2800/5000], Step [8/8], Loss: 0.1374\n",
      "Epoch [2900/5000], Step [8/8], Loss: 0.1972\n",
      "Epoch [3000/5000], Step [8/8], Loss: 0.2200\n",
      "Epoch [3100/5000], Step [8/8], Loss: 0.2068\n",
      "Epoch [3200/5000], Step [8/8], Loss: 0.2083\n",
      "Epoch [3300/5000], Step [8/8], Loss: 0.1906\n",
      "Epoch [3400/5000], Step [8/8], Loss: 0.1840\n",
      "Epoch [3500/5000], Step [8/8], Loss: 0.1821\n",
      "Epoch [3600/5000], Step [8/8], Loss: 0.1782\n",
      "Epoch [3700/5000], Step [8/8], Loss: 0.2311\n",
      "Epoch [3800/5000], Step [8/8], Loss: 0.2540\n",
      "Epoch [3900/5000], Step [8/8], Loss: 0.1871\n",
      "Epoch [4000/5000], Step [8/8], Loss: 0.1684\n",
      "Epoch [4100/5000], Step [8/8], Loss: 0.2220\n",
      "Epoch [4200/5000], Step [8/8], Loss: 0.1720\n",
      "Epoch [4300/5000], Step [8/8], Loss: 0.2070\n",
      "Epoch [4400/5000], Step [8/8], Loss: 0.2099\n",
      "Epoch [4500/5000], Step [8/8], Loss: 0.1952\n",
      "Epoch [4600/5000], Step [8/8], Loss: 0.1493\n",
      "Epoch [4700/5000], Step [8/8], Loss: 0.1777\n",
      "Epoch [4800/5000], Step [8/8], Loss: 0.1830\n",
      "Epoch [4900/5000], Step [8/8], Loss: 0.1963\n",
      "Epoch [5000/5000], Step [8/8], Loss: 0.1685\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 5000 iteration 0\n",
      "Started at 07/28/2019, 23:08:16\n",
      "The time is 07/28/2019, 23:10:49\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [8/8], Loss: 0.2574\n",
      "Epoch [200/5000], Step [8/8], Loss: 0.2286\n",
      "Epoch [300/5000], Step [8/8], Loss: 0.2708\n",
      "Epoch [400/5000], Step [8/8], Loss: 0.1859\n",
      "Epoch [500/5000], Step [8/8], Loss: 0.2091\n",
      "Epoch [600/5000], Step [8/8], Loss: 0.2044\n",
      "Epoch [700/5000], Step [8/8], Loss: 0.2212\n",
      "Epoch [800/5000], Step [8/8], Loss: 0.2362\n",
      "Epoch [900/5000], Step [8/8], Loss: 0.2369\n",
      "Epoch [1000/5000], Step [8/8], Loss: 0.1982\n",
      "Epoch [1100/5000], Step [8/8], Loss: 0.2022\n",
      "Epoch [1200/5000], Step [8/8], Loss: 0.2092\n",
      "Epoch [1300/5000], Step [8/8], Loss: 0.1979\n",
      "Epoch [1400/5000], Step [8/8], Loss: 0.1986\n",
      "Epoch [1500/5000], Step [8/8], Loss: 0.2100\n",
      "Epoch [1600/5000], Step [8/8], Loss: 0.2168\n",
      "Epoch [1700/5000], Step [8/8], Loss: 0.1865\n",
      "Epoch [1800/5000], Step [8/8], Loss: 0.1708\n",
      "Epoch [1900/5000], Step [8/8], Loss: 0.1738\n",
      "Epoch [2000/5000], Step [8/8], Loss: 0.1737\n",
      "Epoch [2100/5000], Step [8/8], Loss: 0.1956\n",
      "Epoch [2200/5000], Step [8/8], Loss: 0.2448\n",
      "Epoch [2300/5000], Step [8/8], Loss: 0.1763\n",
      "Epoch [2400/5000], Step [8/8], Loss: 0.2271\n",
      "Epoch [2500/5000], Step [8/8], Loss: 0.1911\n",
      "Epoch [2600/5000], Step [8/8], Loss: 0.1981\n",
      "Epoch [2700/5000], Step [8/8], Loss: 0.2077\n",
      "Epoch [2800/5000], Step [8/8], Loss: 0.1985\n",
      "Epoch [2900/5000], Step [8/8], Loss: 0.2037\n",
      "Epoch [3000/5000], Step [8/8], Loss: 0.1602\n",
      "Epoch [3100/5000], Step [8/8], Loss: 0.2169\n",
      "Epoch [3200/5000], Step [8/8], Loss: 0.2031\n",
      "Epoch [3300/5000], Step [8/8], Loss: 0.1714\n",
      "Epoch [3400/5000], Step [8/8], Loss: 0.1712\n",
      "Epoch [3500/5000], Step [8/8], Loss: 0.2367\n",
      "Epoch [3600/5000], Step [8/8], Loss: 0.2085\n",
      "Epoch [3700/5000], Step [8/8], Loss: 0.2281\n",
      "Epoch [3800/5000], Step [8/8], Loss: 0.1603\n",
      "Epoch [3900/5000], Step [8/8], Loss: 0.1915\n",
      "Epoch [4000/5000], Step [8/8], Loss: 0.1793\n",
      "Epoch [4100/5000], Step [8/8], Loss: 0.1712\n",
      "Epoch [4200/5000], Step [8/8], Loss: 0.1818\n",
      "Epoch [4300/5000], Step [8/8], Loss: 0.1552\n",
      "Epoch [4400/5000], Step [8/8], Loss: 0.2052\n",
      "Epoch [4500/5000], Step [8/8], Loss: 0.1898\n",
      "Epoch [4600/5000], Step [8/8], Loss: 0.2048\n",
      "Epoch [4700/5000], Step [8/8], Loss: 0.1921\n",
      "Epoch [4800/5000], Step [8/8], Loss: 0.1943\n",
      "Epoch [4900/5000], Step [8/8], Loss: 0.2019\n",
      "Epoch [5000/5000], Step [8/8], Loss: 0.1937\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 5000 iteration 1\n",
      "Started at 07/28/2019, 23:10:49\n",
      "The time is 07/28/2019, 23:13:20\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [8/8], Loss: 0.1862\n",
      "Epoch [200/5000], Step [8/8], Loss: 0.2042\n",
      "Epoch [300/5000], Step [8/8], Loss: 0.2391\n",
      "Epoch [400/5000], Step [8/8], Loss: 0.2411\n",
      "Epoch [500/5000], Step [8/8], Loss: 0.2044\n",
      "Epoch [600/5000], Step [8/8], Loss: 0.2218\n",
      "Epoch [700/5000], Step [8/8], Loss: 0.2061\n",
      "Epoch [800/5000], Step [8/8], Loss: 0.1921\n",
      "Epoch [900/5000], Step [8/8], Loss: 0.2056\n",
      "Epoch [1000/5000], Step [8/8], Loss: 0.2059\n",
      "Epoch [1100/5000], Step [8/8], Loss: 0.1917\n",
      "Epoch [1200/5000], Step [8/8], Loss: 0.2134\n",
      "Epoch [1300/5000], Step [8/8], Loss: 0.2305\n",
      "Epoch [1400/5000], Step [8/8], Loss: 0.1894\n",
      "Epoch [1500/5000], Step [8/8], Loss: 0.2221\n",
      "Epoch [1600/5000], Step [8/8], Loss: 0.2167\n",
      "Epoch [1700/5000], Step [8/8], Loss: 0.2350\n",
      "Epoch [1800/5000], Step [8/8], Loss: 0.1906\n",
      "Epoch [1900/5000], Step [8/8], Loss: 0.1735\n",
      "Epoch [2000/5000], Step [8/8], Loss: 0.1870\n",
      "Epoch [2100/5000], Step [8/8], Loss: 0.1876\n",
      "Epoch [2200/5000], Step [8/8], Loss: 0.2304\n",
      "Epoch [2300/5000], Step [8/8], Loss: 0.2184\n",
      "Epoch [2400/5000], Step [8/8], Loss: 0.1834\n",
      "Epoch [2500/5000], Step [8/8], Loss: 0.1843\n",
      "Epoch [2600/5000], Step [8/8], Loss: 0.1815\n",
      "Epoch [2700/5000], Step [8/8], Loss: 0.1788\n",
      "Epoch [2800/5000], Step [8/8], Loss: 0.1933\n",
      "Epoch [2900/5000], Step [8/8], Loss: 0.1763\n",
      "Epoch [3000/5000], Step [8/8], Loss: 0.1729\n",
      "Epoch [3100/5000], Step [8/8], Loss: 0.2086\n",
      "Epoch [3200/5000], Step [8/8], Loss: 0.2064\n",
      "Epoch [3300/5000], Step [8/8], Loss: 0.2006\n",
      "Epoch [3400/5000], Step [8/8], Loss: 0.1922\n",
      "Epoch [3500/5000], Step [8/8], Loss: 0.1751\n",
      "Epoch [3600/5000], Step [8/8], Loss: 0.2079\n",
      "Epoch [3700/5000], Step [8/8], Loss: 0.2109\n",
      "Epoch [3800/5000], Step [8/8], Loss: 0.1608\n",
      "Epoch [3900/5000], Step [8/8], Loss: 0.1855\n",
      "Epoch [4000/5000], Step [8/8], Loss: 0.2022\n",
      "Epoch [4100/5000], Step [8/8], Loss: 0.1516\n",
      "Epoch [4200/5000], Step [8/8], Loss: 0.2063\n",
      "Epoch [4300/5000], Step [8/8], Loss: 0.2434\n",
      "Epoch [4400/5000], Step [8/8], Loss: 0.2038\n",
      "Epoch [4500/5000], Step [8/8], Loss: 0.1999\n",
      "Epoch [4600/5000], Step [8/8], Loss: 0.1889\n",
      "Epoch [4700/5000], Step [8/8], Loss: 0.1937\n",
      "Epoch [4800/5000], Step [8/8], Loss: 0.1839\n",
      "Epoch [4900/5000], Step [8/8], Loss: 0.2055\n",
      "Epoch [5000/5000], Step [8/8], Loss: 0.1915\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 5000 iteration 2\n",
      "Started at 07/28/2019, 23:13:20\n",
      "The time is 07/28/2019, 23:15:52\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [8/8], Loss: 0.2827\n",
      "Epoch [200/5000], Step [8/8], Loss: 0.1763\n",
      "Epoch [300/5000], Step [8/8], Loss: 0.2414\n",
      "Epoch [400/5000], Step [8/8], Loss: 0.1710\n",
      "Epoch [500/5000], Step [8/8], Loss: 0.2528\n",
      "Epoch [600/5000], Step [8/8], Loss: 0.1949\n",
      "Epoch [700/5000], Step [8/8], Loss: 0.2617\n",
      "Epoch [800/5000], Step [8/8], Loss: 0.1998\n",
      "Epoch [900/5000], Step [8/8], Loss: 0.1653\n",
      "Epoch [1000/5000], Step [8/8], Loss: 0.2138\n",
      "Epoch [1100/5000], Step [8/8], Loss: 0.2020\n",
      "Epoch [1200/5000], Step [8/8], Loss: 0.2018\n",
      "Epoch [1300/5000], Step [8/8], Loss: 0.1907\n",
      "Epoch [1400/5000], Step [8/8], Loss: 0.2305\n",
      "Epoch [1500/5000], Step [8/8], Loss: 0.1746\n",
      "Epoch [1600/5000], Step [8/8], Loss: 0.2226\n",
      "Epoch [1700/5000], Step [8/8], Loss: 0.2241\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1800/5000], Step [8/8], Loss: 0.2003\n",
      "Epoch [1900/5000], Step [8/8], Loss: 0.1604\n",
      "Epoch [2000/5000], Step [8/8], Loss: 0.1984\n",
      "Epoch [2100/5000], Step [8/8], Loss: 0.2161\n",
      "Epoch [2200/5000], Step [8/8], Loss: 0.2134\n",
      "Epoch [2300/5000], Step [8/8], Loss: 0.2457\n",
      "Epoch [2400/5000], Step [8/8], Loss: 0.2196\n",
      "Epoch [2500/5000], Step [8/8], Loss: 0.2384\n",
      "Epoch [2600/5000], Step [8/8], Loss: 0.1819\n",
      "Epoch [2700/5000], Step [8/8], Loss: 0.1798\n",
      "Epoch [2800/5000], Step [8/8], Loss: 0.1619\n",
      "Epoch [2900/5000], Step [8/8], Loss: 0.1883\n",
      "Epoch [3000/5000], Step [8/8], Loss: 0.1793\n",
      "Epoch [3100/5000], Step [8/8], Loss: 0.1871\n",
      "Epoch [3200/5000], Step [8/8], Loss: 0.1958\n",
      "Epoch [3300/5000], Step [8/8], Loss: 0.2151\n",
      "Epoch [3400/5000], Step [8/8], Loss: 0.1955\n",
      "Epoch [3500/5000], Step [8/8], Loss: 0.2141\n",
      "Epoch [3600/5000], Step [8/8], Loss: 0.2135\n",
      "Epoch [3700/5000], Step [8/8], Loss: 0.1968\n",
      "Epoch [3800/5000], Step [8/8], Loss: 0.1984\n",
      "Epoch [3900/5000], Step [8/8], Loss: 0.2316\n",
      "Epoch [4000/5000], Step [8/8], Loss: 0.2038\n",
      "Epoch [4100/5000], Step [8/8], Loss: 0.2134\n",
      "Epoch [4200/5000], Step [8/8], Loss: 0.1890\n",
      "Epoch [4300/5000], Step [8/8], Loss: 0.2247\n",
      "Epoch [4400/5000], Step [8/8], Loss: 0.1922\n",
      "Epoch [4500/5000], Step [8/8], Loss: 0.1872\n",
      "Epoch [4600/5000], Step [8/8], Loss: 0.1940\n",
      "Epoch [4700/5000], Step [8/8], Loss: 0.2013\n",
      "Epoch [4800/5000], Step [8/8], Loss: 0.2015\n",
      "Epoch [4900/5000], Step [8/8], Loss: 0.1724\n",
      "Epoch [5000/5000], Step [8/8], Loss: 0.2314\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 5000 iteration 3\n",
      "Started at 07/28/2019, 23:15:52\n",
      "The time is 07/28/2019, 23:18:24\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [8/8], Loss: 0.2243\n",
      "Epoch [200/5000], Step [8/8], Loss: 0.2173\n",
      "Epoch [300/5000], Step [8/8], Loss: 0.2153\n",
      "Epoch [400/5000], Step [8/8], Loss: 0.2359\n",
      "Epoch [500/5000], Step [8/8], Loss: 0.1735\n",
      "Epoch [600/5000], Step [8/8], Loss: 0.1707\n",
      "Epoch [700/5000], Step [8/8], Loss: 0.2147\n",
      "Epoch [800/5000], Step [8/8], Loss: 0.1843\n",
      "Epoch [900/5000], Step [8/8], Loss: 0.2586\n",
      "Epoch [1000/5000], Step [8/8], Loss: 0.1712\n",
      "Epoch [1100/5000], Step [8/8], Loss: 0.1886\n",
      "Epoch [1200/5000], Step [8/8], Loss: 0.2217\n",
      "Epoch [1300/5000], Step [8/8], Loss: 0.1679\n",
      "Epoch [1400/5000], Step [8/8], Loss: 0.2145\n",
      "Epoch [1500/5000], Step [8/8], Loss: 0.2312\n",
      "Epoch [1600/5000], Step [8/8], Loss: 0.2094\n",
      "Epoch [1700/5000], Step [8/8], Loss: 0.2368\n",
      "Epoch [1800/5000], Step [8/8], Loss: 0.2555\n",
      "Epoch [1900/5000], Step [8/8], Loss: 0.2097\n",
      "Epoch [2000/5000], Step [8/8], Loss: 0.1634\n",
      "Epoch [2100/5000], Step [8/8], Loss: 0.1768\n",
      "Epoch [2200/5000], Step [8/8], Loss: 0.2191\n",
      "Epoch [2300/5000], Step [8/8], Loss: 0.1882\n",
      "Epoch [2400/5000], Step [8/8], Loss: 0.1986\n",
      "Epoch [2500/5000], Step [8/8], Loss: 0.2232\n",
      "Epoch [2600/5000], Step [8/8], Loss: 0.1956\n",
      "Epoch [2700/5000], Step [8/8], Loss: 0.1868\n",
      "Epoch [2800/5000], Step [8/8], Loss: 0.1779\n",
      "Epoch [2900/5000], Step [8/8], Loss: 0.2260\n",
      "Epoch [3000/5000], Step [8/8], Loss: 0.2004\n",
      "Epoch [3100/5000], Step [8/8], Loss: 0.1940\n",
      "Epoch [3200/5000], Step [8/8], Loss: 0.1966\n",
      "Epoch [3300/5000], Step [8/8], Loss: 0.2034\n",
      "Epoch [3400/5000], Step [8/8], Loss: 0.1706\n",
      "Epoch [3500/5000], Step [8/8], Loss: 0.1803\n",
      "Epoch [3600/5000], Step [8/8], Loss: 0.2045\n",
      "Epoch [3700/5000], Step [8/8], Loss: 0.2040\n",
      "Epoch [3800/5000], Step [8/8], Loss: 0.1677\n",
      "Epoch [3900/5000], Step [8/8], Loss: 0.2226\n",
      "Epoch [4000/5000], Step [8/8], Loss: 0.1932\n",
      "Epoch [4100/5000], Step [8/8], Loss: 0.1930\n",
      "Epoch [4200/5000], Step [8/8], Loss: 0.1770\n",
      "Epoch [4300/5000], Step [8/8], Loss: 0.1734\n",
      "Epoch [4400/5000], Step [8/8], Loss: 0.1875\n",
      "Epoch [4500/5000], Step [8/8], Loss: 0.2558\n",
      "Epoch [4600/5000], Step [8/8], Loss: 0.1749\n",
      "Epoch [4700/5000], Step [8/8], Loss: 0.1729\n",
      "Epoch [4800/5000], Step [8/8], Loss: 0.1836\n",
      "Epoch [4900/5000], Step [8/8], Loss: 0.1998\n",
      "Epoch [5000/5000], Step [8/8], Loss: 0.2564\n",
      "################################################\n",
      "################################################\n",
      "Have finished dsSize 5000 iteration 4\n",
      "Started at 07/28/2019, 23:18:24\n",
      "The time is 07/28/2019, 23:20:56\n",
      "################################################\n",
      "################################################\n",
      "Epoch [100/5000], Step [16/16], Loss: 0.2637\n",
      "Epoch [200/5000], Step [16/16], Loss: 0.2300\n",
      "Epoch [300/5000], Step [16/16], Loss: 0.2024\n",
      "Epoch [400/5000], Step [16/16], Loss: 0.1805\n",
      "Epoch [500/5000], Step [16/16], Loss: 0.2170\n",
      "Epoch [600/5000], Step [16/16], Loss: 0.1938\n",
      "Epoch [700/5000], Step [16/16], Loss: 0.1992\n",
      "Epoch [800/5000], Step [16/16], Loss: 0.2162\n",
      "Epoch [900/5000], Step [16/16], Loss: 0.1926\n",
      "Epoch [1000/5000], Step [16/16], Loss: 0.2042\n",
      "Epoch [1100/5000], Step [16/16], Loss: 0.2571\n",
      "Epoch [1200/5000], Step [16/16], Loss: 0.1703\n",
      "Epoch [1300/5000], Step [16/16], Loss: 0.2043\n",
      "Epoch [1400/5000], Step [16/16], Loss: 0.1885\n",
      "Epoch [1500/5000], Step [16/16], Loss: 0.2108\n",
      "Epoch [1600/5000], Step [16/16], Loss: 0.1978\n",
      "Epoch [1700/5000], Step [16/16], Loss: 0.1609\n",
      "Epoch [1800/5000], Step [16/16], Loss: 0.2053\n",
      "Epoch [1900/5000], Step [16/16], Loss: 0.1900\n",
      "Epoch [2000/5000], Step [16/16], Loss: 0.1935\n",
      "Epoch [2100/5000], Step [16/16], Loss: 0.1941\n",
      "Epoch [2200/5000], Step [16/16], Loss: 0.2300\n",
      "Epoch [2300/5000], Step [16/16], Loss: 0.1979\n",
      "Epoch [2400/5000], Step [16/16], Loss: 0.2130\n",
      "Epoch [2500/5000], Step [16/16], Loss: 0.2542\n",
      "Epoch [2600/5000], Step [16/16], Loss: 0.2314\n",
      "Epoch [2700/5000], Step [16/16], Loss: 0.2174\n",
      "Epoch [2800/5000], Step [16/16], Loss: 0.1998\n",
      "Epoch [2900/5000], Step [16/16], Loss: 0.2119\n",
      "Epoch [3000/5000], Step [16/16], Loss: 0.1998\n",
      "Epoch [3100/5000], Step [16/16], Loss: 0.2092\n",
      "Epoch [3200/5000], Step [16/16], Loss: 0.2650\n"
     ]
    }
   ],
   "source": [
    "for dsSize in datasetSizes:\n",
    "    for iteration_no in range(5):\n",
    "        \n",
    "        X_train, X_validate, y_train, y_validate = load_dataset(os.path.join(datasetLocation, datasetFileTemplate.format(dsSize)))\n",
    "        X_train = torch.tensor(X_train).float()\n",
    "        X_validate = torch.tensor(X_validate).float()\n",
    "        y_train = torch.tensor(y_train).float()\n",
    "        y_validate = torch.tensor(y_validate).float()\n",
    "        \n",
    "        train_data = TabularDataset(X_train, y_train)\n",
    "        train_loader = DataLoader(train_data, \n",
    "                                   batch_size=batch_size, \n",
    "                                   shuffle=True)\n",
    "        \n",
    "        validate_data = TabularDataset(X_validate, y_validate)\n",
    "        validate_loader = DataLoader(dataset = validate_data,\n",
    "                                     batch_size=batch_size, \n",
    "                                     shuffle=False)\n",
    "        \n",
    "        total_step = len(train_loader)\n",
    "\n",
    "        saveLocation = saveLocationTemplate.format(dsSize, iteration_no)\n",
    "\n",
    "\n",
    "        my_random_seed += 1\n",
    "        random.seed(my_random_seed)\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        \n",
    "        model = NeuralNet(6, 1).to(device)\n",
    "\n",
    "        # Loss and optimizer\n",
    "        criterion = nn.BCEWithLogitsLoss().to(device)\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)  \n",
    "\n",
    "        # Train the model\n",
    "        for epoch in range(num_epochs):\n",
    "            for i, (xs, ys) in enumerate(train_loader):  \n",
    "                # Move tensors to the configured device\n",
    "                xs = xs.to(device)\n",
    "                ys = ys.view(-1, 1).to(device)\n",
    "                \n",
    "                # Forward pass\n",
    "                outputs = model(xs)\n",
    "                loss = criterion(outputs, ys)\n",
    "\n",
    "                # Backward and optimize\n",
    "                optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if (epoch+1) % 100 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}' \n",
    "                       .format(epoch+1, num_epochs, i+1, total_step, loss.item()))\n",
    "                \n",
    "#         with torch.no_grad():\n",
    "#             correct = 0\n",
    "#             total = 0\n",
    "#             for xs, ys in test_loader:\n",
    "#                 xs = xs.to(device)\n",
    "#                 ys = ys.view(-1, 1).to(device)\n",
    "#                 outputs = model(xs)\n",
    "#                 _, predicted = torch.round(outputs.data, 1)\n",
    "#                 total += labels.size(0)\n",
    "#                 correct += (predicted == ys).sum().item()\n",
    "        if not os.path.exists(saveLocation):\n",
    "            os.makedirs(saveLocation)\n",
    "        torch.save(model.state_dict(), os.path.join(saveLocation,'model.ckpt'))\n",
    "\n",
    "        end_time = datetime.now()\n",
    "\n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "        print(\"Have finished dsSize {} iteration {}\".format(dsSize, iteration_no))\n",
    "        print(\"Started at {}\".format(start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        print(\"The time is {}\".format(end_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "NeuralNet(\n",
       "  (fc1): Linear(in_features=6, out_features=16, bias=True)\n",
       "  (fc2): Linear(in_features=16, out_features=16, bias=True)\n",
       "  (fc3): Linear(in_features=16, out_features=1, bias=True)\n",
       "  (relu): ReLU()\n",
       "  (sigmoid): Sigmoid()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-5.3766],\n",
       "        [-5.3431],\n",
       "        [-3.4633],\n",
       "        [-4.0969],\n",
       "        [-2.0781],\n",
       "        [-2.6340],\n",
       "        [-1.3242],\n",
       "        [-0.7078],\n",
       "        [-5.3072],\n",
       "        [-2.8817],\n",
       "        [-2.7381],\n",
       "        [-2.6294],\n",
       "        [-3.9931],\n",
       "        [-2.7070],\n",
       "        [-4.2784],\n",
       "        [-2.4114],\n",
       "        [-4.9203],\n",
       "        [-1.5695],\n",
       "        [-4.0730],\n",
       "        [-2.7006],\n",
       "        [-5.5958],\n",
       "        [-2.9969],\n",
       "        [-2.4064],\n",
       "        [-3.5870],\n",
       "        [-4.2876],\n",
       "        [-4.1803],\n",
       "        [-6.1838],\n",
       "        [-1.6297],\n",
       "        [-4.2251],\n",
       "        [-4.9519],\n",
       "        [-2.7264],\n",
       "        [-4.9742],\n",
       "        [-5.4956],\n",
       "        [-3.4699],\n",
       "        [-2.3981],\n",
       "        [-1.8624],\n",
       "        [-2.0562],\n",
       "        [-3.7674],\n",
       "        [-1.2754],\n",
       "        [-5.6785],\n",
       "        [-0.4536],\n",
       "        [-4.1079],\n",
       "        [-3.4017],\n",
       "        [-1.9427],\n",
       "        [-2.9994],\n",
       "        [-5.6323],\n",
       "        [-1.5211],\n",
       "        [-6.1970],\n",
       "        [-5.8131],\n",
       "        [-1.7056],\n",
       "        [-2.5823],\n",
       "        [-2.9177],\n",
       "        [-4.8693],\n",
       "        [-2.3777],\n",
       "        [-4.7885],\n",
       "        [-3.3755],\n",
       "        [-2.6658],\n",
       "        [-2.8084],\n",
       "        [-3.8573],\n",
       "        [-6.3865],\n",
       "        [-3.2304],\n",
       "        [-3.6780],\n",
       "        [-4.5869],\n",
       "        [-2.2606],\n",
       "        [-3.8578],\n",
       "        [-2.0229],\n",
       "        [-2.5173],\n",
       "        [-2.0587],\n",
       "        [-4.2479],\n",
       "        [-5.3848],\n",
       "        [-4.8761],\n",
       "        [-4.7862],\n",
       "        [-3.4616],\n",
       "        [-1.9783],\n",
       "        [-2.5362],\n",
       "        [-4.8980],\n",
       "        [-1.4026],\n",
       "        [-2.4663],\n",
       "        [-1.9941],\n",
       "        [-3.6442],\n",
       "        [-2.5178],\n",
       "        [-3.7116],\n",
       "        [-3.1844],\n",
       "        [-2.0895],\n",
       "        [-3.0181],\n",
       "        [-2.0403],\n",
       "        [-5.6441],\n",
       "        [-3.1529],\n",
       "        [-2.2540],\n",
       "        [-4.6509],\n",
       "        [-4.5946],\n",
       "        [-3.7912],\n",
       "        [-3.1733],\n",
       "        [-2.1885],\n",
       "        [-1.7703],\n",
       "        [-2.2449],\n",
       "        [-4.3529],\n",
       "        [-3.3747],\n",
       "        [-2.5266],\n",
       "        [-3.2952],\n",
       "        [-1.1264],\n",
       "        [-4.6129],\n",
       "        [-4.7265],\n",
       "        [-2.8792],\n",
       "        [-3.6280],\n",
       "        [-3.4543],\n",
       "        [-2.0187],\n",
       "        [-3.4140],\n",
       "        [-4.5696],\n",
       "        [-4.2962],\n",
       "        [-2.9063],\n",
       "        [-2.7761],\n",
       "        [-3.9053],\n",
       "        [-1.1070],\n",
       "        [-3.2140],\n",
       "        [-2.0809],\n",
       "        [-4.6407],\n",
       "        [-3.8877],\n",
       "        [-4.8214],\n",
       "        [-1.5198],\n",
       "        [-2.1383],\n",
       "        [-4.0369],\n",
       "        [-1.3494],\n",
       "        [-3.2183],\n",
       "        [-3.9508],\n",
       "        [-3.3967],\n",
       "        [-4.8187],\n",
       "        [-1.4176],\n",
       "        [-2.6974],\n",
       "        [-1.8124],\n",
       "        [-4.6584],\n",
       "        [-0.7309],\n",
       "        [-1.6135],\n",
       "        [-3.8167],\n",
       "        [-2.3268],\n",
       "        [-2.8575],\n",
       "        [-2.8859],\n",
       "        [-4.2907],\n",
       "        [-3.0038],\n",
       "        [-5.2846],\n",
       "        [-6.9037],\n",
       "        [-4.5288],\n",
       "        [-2.7242],\n",
       "        [-1.0463],\n",
       "        [-1.5211],\n",
       "        [-2.9781],\n",
       "        [-1.1366],\n",
       "        [-4.6504],\n",
       "        [-1.7419],\n",
       "        [-2.2087],\n",
       "        [-2.6325],\n",
       "        [-1.5693],\n",
       "        [-3.7666],\n",
       "        [-5.2677],\n",
       "        [-5.7140],\n",
       "        [-1.7922],\n",
       "        [-2.0200],\n",
       "        [-4.2402],\n",
       "        [-3.4295],\n",
       "        [-2.4972],\n",
       "        [-5.1940],\n",
       "        [-1.4402],\n",
       "        [-4.5958],\n",
       "        [-5.2456],\n",
       "        [-3.3972],\n",
       "        [-2.5870],\n",
       "        [-1.7138],\n",
       "        [-2.3936],\n",
       "        [-1.7932],\n",
       "        [-2.2561],\n",
       "        [-2.6609],\n",
       "        [-3.6163],\n",
       "        [-5.0989],\n",
       "        [-5.6071],\n",
       "        [-4.7258],\n",
       "        [-2.5695],\n",
       "        [-2.9955],\n",
       "        [-3.3373],\n",
       "        [-3.1195],\n",
       "        [-2.9277],\n",
       "        [-1.5255],\n",
       "        [-5.3518],\n",
       "        [-3.5531],\n",
       "        [-1.7640],\n",
       "        [-3.6467],\n",
       "        [-2.7273],\n",
       "        [-3.1557],\n",
       "        [-2.3674],\n",
       "        [-3.4878],\n",
       "        [-1.3156],\n",
       "        [-4.8498],\n",
       "        [-2.7601],\n",
       "        [-5.1739],\n",
       "        [-4.7787],\n",
       "        [-2.4539],\n",
       "        [-2.2291],\n",
       "        [-3.9848],\n",
       "        [-5.2360],\n",
       "        [-3.1332],\n",
       "        [-4.4326],\n",
       "        [-2.5470],\n",
       "        [-1.1569],\n",
       "        [-0.6991],\n",
       "        [-3.7469],\n",
       "        [-1.4804],\n",
       "        [-5.9604],\n",
       "        [-2.4827],\n",
       "        [-5.0796],\n",
       "        [-3.7699],\n",
       "        [-2.1721],\n",
       "        [-2.1535],\n",
       "        [-0.7303],\n",
       "        [-1.3867],\n",
       "        [-0.8552],\n",
       "        [-3.0669],\n",
       "        [-7.5882],\n",
       "        [-2.5884],\n",
       "        [-2.1508],\n",
       "        [-0.3519],\n",
       "        [-5.7520],\n",
       "        [-6.5089],\n",
       "        [-6.1626],\n",
       "        [-2.6882],\n",
       "        [-0.9705],\n",
       "        [-4.0496],\n",
       "        [-1.5464],\n",
       "        [-3.1576],\n",
       "        [-4.9382],\n",
       "        [-2.5694],\n",
       "        [-2.8421],\n",
       "        [-1.2765],\n",
       "        [-3.3933],\n",
       "        [-3.4049],\n",
       "        [-3.8123],\n",
       "        [-3.2297],\n",
       "        [-3.2356],\n",
       "        [-2.2840],\n",
       "        [-3.9149],\n",
       "        [-4.8079],\n",
       "        [-1.7967],\n",
       "        [-2.8056],\n",
       "        [-4.1988],\n",
       "        [-1.8505],\n",
       "        [-2.0768],\n",
       "        [-5.4910],\n",
       "        [-1.5599],\n",
       "        [-4.4842],\n",
       "        [-3.6057],\n",
       "        [-3.1601],\n",
       "        [-4.9954],\n",
       "        [-2.9201],\n",
       "        [-3.8236],\n",
       "        [-3.0536],\n",
       "        [-3.0515],\n",
       "        [-5.5692],\n",
       "        [-1.3388],\n",
       "        [-4.8519],\n",
       "        [-4.7929],\n",
       "        [-5.7054],\n",
       "        [-4.5001],\n",
       "        [-3.1332],\n",
       "        [-2.5662],\n",
       "        [-4.8326],\n",
       "        [-3.7924],\n",
       "        [-3.7307],\n",
       "        [-2.3343],\n",
       "        [-3.0466],\n",
       "        [-4.4505],\n",
       "        [-2.1918],\n",
       "        [-1.1060],\n",
       "        [-0.6105],\n",
       "        [-3.2351],\n",
       "        [-3.1196],\n",
       "        [-6.7352],\n",
       "        [-2.7575],\n",
       "        [-1.9572],\n",
       "        [-3.8068],\n",
       "        [-2.8166],\n",
       "        [-6.0195],\n",
       "        [-2.5537],\n",
       "        [-1.4264],\n",
       "        [-3.9649],\n",
       "        [-4.4558],\n",
       "        [-1.4269],\n",
       "        [-6.2951],\n",
       "        [-4.8520],\n",
       "        [-3.4068],\n",
       "        [-6.5842],\n",
       "        [-3.1579],\n",
       "        [-1.5786],\n",
       "        [-6.6041],\n",
       "        [-5.6898],\n",
       "        [-5.3529],\n",
       "        [-3.7149],\n",
       "        [-2.8827],\n",
       "        [-3.7796],\n",
       "        [-2.2733],\n",
       "        [-2.9085],\n",
       "        [-5.7960],\n",
       "        [-2.1413],\n",
       "        [-1.9169],\n",
       "        [-0.9469],\n",
       "        [-3.5168],\n",
       "        [-1.5656],\n",
       "        [-2.4630],\n",
       "        [-2.1534],\n",
       "        [-6.4754],\n",
       "        [-3.8266],\n",
       "        [-5.2562],\n",
       "        [-6.9975],\n",
       "        [-1.1496],\n",
       "        [-1.9858],\n",
       "        [-4.0699],\n",
       "        [-2.6802],\n",
       "        [-3.1934],\n",
       "        [-4.8558],\n",
       "        [-3.4841],\n",
       "        [-4.5259],\n",
       "        [-2.2883],\n",
       "        [-3.9756],\n",
       "        [-3.0531],\n",
       "        [-3.0466],\n",
       "        [-4.1749],\n",
       "        [-4.7161],\n",
       "        [-4.6680],\n",
       "        [-3.4523],\n",
       "        [-2.6252],\n",
       "        [-3.8376],\n",
       "        [-1.8562],\n",
       "        [-1.3896],\n",
       "        [-3.9971],\n",
       "        [-4.6286],\n",
       "        [-5.7378],\n",
       "        [-1.4557],\n",
       "        [-0.9020],\n",
       "        [-3.8905],\n",
       "        [-1.8073],\n",
       "        [-5.3327],\n",
       "        [-3.0337],\n",
       "        [-3.2858],\n",
       "        [-4.3383],\n",
       "        [-7.1101],\n",
       "        [-2.4995],\n",
       "        [-2.8916],\n",
       "        [-3.4007],\n",
       "        [-1.8500],\n",
       "        [-2.5630],\n",
       "        [-5.2987],\n",
       "        [-5.2623],\n",
       "        [-5.5442],\n",
       "        [-4.4544],\n",
       "        [-4.1233],\n",
       "        [-4.9681],\n",
       "        [-4.5925],\n",
       "        [-2.5794],\n",
       "        [-5.7654],\n",
       "        [-6.0854],\n",
       "        [-2.8480],\n",
       "        [-2.6176],\n",
       "        [-1.8040],\n",
       "        [-2.7261],\n",
       "        [-1.9768],\n",
       "        [-2.4952],\n",
       "        [-2.6427],\n",
       "        [-2.7546],\n",
       "        [-2.1452],\n",
       "        [-2.5480],\n",
       "        [-0.8899],\n",
       "        [-6.5862],\n",
       "        [-3.5455],\n",
       "        [-1.8147],\n",
       "        [-2.9535],\n",
       "        [-3.1534],\n",
       "        [-3.0143],\n",
       "        [-6.7846],\n",
       "        [-2.6442],\n",
       "        [-1.5721],\n",
       "        [-2.7222],\n",
       "        [-2.6036],\n",
       "        [-3.2081],\n",
       "        [-4.6794],\n",
       "        [-4.1407],\n",
       "        [-1.3222],\n",
       "        [-2.4587],\n",
       "        [-5.5400],\n",
       "        [-5.3879],\n",
       "        [-2.3832],\n",
       "        [-2.8345],\n",
       "        [-2.8559],\n",
       "        [-2.2120],\n",
       "        [-1.8414],\n",
       "        [-1.7225],\n",
       "        [-2.8344],\n",
       "        [-3.7610],\n",
       "        [-2.5377],\n",
       "        [-3.2490],\n",
       "        [-2.9720],\n",
       "        [-2.7332],\n",
       "        [-2.5461],\n",
       "        [-1.6414],\n",
       "        [-2.4128],\n",
       "        [-2.8938],\n",
       "        [-3.1308],\n",
       "        [-1.4341],\n",
       "        [-4.6597],\n",
       "        [-4.8512],\n",
       "        [-2.7297],\n",
       "        [-2.8905],\n",
       "        [-3.3343],\n",
       "        [-2.3674],\n",
       "        [-3.4384],\n",
       "        [-2.7198],\n",
       "        [-2.8782],\n",
       "        [-2.8354],\n",
       "        [-1.5336],\n",
       "        [-2.4525],\n",
       "        [-2.3637],\n",
       "        [-2.8599],\n",
       "        [-3.3363],\n",
       "        [-2.0107],\n",
       "        [-3.0132],\n",
       "        [-5.0394],\n",
       "        [-2.0716],\n",
       "        [-4.7218],\n",
       "        [-1.6362],\n",
       "        [-5.6469],\n",
       "        [-1.3180],\n",
       "        [-1.7549],\n",
       "        [-3.0325],\n",
       "        [-1.4742],\n",
       "        [-3.1717],\n",
       "        [-2.7500],\n",
       "        [-2.3241],\n",
       "        [-6.0136],\n",
       "        [-5.5421],\n",
       "        [-3.8214],\n",
       "        [-6.4091],\n",
       "        [-4.6646],\n",
       "        [-2.7824],\n",
       "        [-2.2526],\n",
       "        [-1.4121],\n",
       "        [-4.9797],\n",
       "        [-2.4846],\n",
       "        [-4.3809],\n",
       "        [-4.5894],\n",
       "        [-2.2673],\n",
       "        [-3.4962],\n",
       "        [-4.1710],\n",
       "        [-3.4211],\n",
       "        [-7.2976],\n",
       "        [-2.5958],\n",
       "        [-3.8874],\n",
       "        [-2.1815],\n",
       "        [-5.4231],\n",
       "        [-3.3031],\n",
       "        [-6.3832],\n",
       "        [-5.0787],\n",
       "        [-2.5152],\n",
       "        [-1.9478],\n",
       "        [-4.3374],\n",
       "        [-3.5795],\n",
       "        [-2.2343],\n",
       "        [-2.8696],\n",
       "        [-4.6696],\n",
       "        [-1.7105],\n",
       "        [-2.7138],\n",
       "        [-2.8580],\n",
       "        [-2.6723],\n",
       "        [-3.4109],\n",
       "        [-2.8080],\n",
       "        [-4.3414],\n",
       "        [-1.0284],\n",
       "        [-3.2771],\n",
       "        [-2.7235],\n",
       "        [-3.0073],\n",
       "        [-3.5609],\n",
       "        [-2.7982],\n",
       "        [-4.7349],\n",
       "        [-6.2918],\n",
       "        [-3.8402],\n",
       "        [-1.6463],\n",
       "        [-0.9316],\n",
       "        [-0.8212],\n",
       "        [-5.8502],\n",
       "        [-1.4610],\n",
       "        [-2.1299],\n",
       "        [-2.2169],\n",
       "        [-4.8566],\n",
       "        [-1.9586],\n",
       "        [-2.1590],\n",
       "        [-1.4648],\n",
       "        [-3.2988],\n",
       "        [-2.9458],\n",
       "        [-5.3362],\n",
       "        [-5.3669],\n",
       "        [-3.2810],\n",
       "        [-3.5525],\n",
       "        [-3.2132],\n",
       "        [-5.0171],\n",
       "        [-1.6264]], device='cuda:1', grad_fn=<AddmmBackward>)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig = nn.Sigmoid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sig_out = sig(outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpuys = ys.cpu().numpy() + 1\n",
    "cpuout = sig_out.cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8545115222330413"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "fpr, tpr, thresholds = metrics.roc_curve(cpuys, cpuout, pos_label=2)\n",
    "metrics.auc(fpr, tpr)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure()\n",
    "lw = 2\n",
    "plt.plot(fpr, tpr, color='darkorange',\n",
    "         lw=lw, label='ROC curve (area = %0.2f)' % metrics.auc(fpr, tpr))\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver operating characteristic example')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
