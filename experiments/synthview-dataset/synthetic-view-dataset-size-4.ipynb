{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "import neat\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "from explaneat.core.backprop import NeatNet\n",
    "from explaneat.core import backprop\n",
    "from explaneat.core.backproppop import BackpropPopulation\n",
    "from explaneat.visualization import visualize\n",
    "from explaneat.core.experiment import ExperimentReporter\n",
    "from explaneat.core.utility import one_hot_encode\n",
    "\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn import metrics\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Breast Cancer Experiment\n",
    "\n",
    "This experiment (a) test the experimental environment, but is also to evaluate the efficacy of the ExplaNEAT algorithm. Speed is a critical factor, as well as stability of results on population size. Total run time will also be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a random seed and a total stopping point in the number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_random_seed = 42\n",
    "random.seed(my_random_seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot_encode(vals):\n",
    "    width = max(vals)\n",
    "    newVals = []\n",
    "    for val in vals:\n",
    "        blank = [0. for _ in range(width + 1)]\n",
    "        blank[val] = 1.\n",
    "        newVals.append(blank)\n",
    "    return np.asarray(newVals)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to work with the Iris dataset, which will be loaded from `sklearn`. We want to characterise the efficacy of the algorithm with regards to a mostly untransformed dataset, so we will only normalise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(fp, \n",
    "                 randomSeed = 42, \n",
    "                 proportionValidation = 0.2):\n",
    "    ''' Takes in a filepath, returns x_train, x_validate, y_train, y_validate'''\n",
    "    df = pd.read_csv(fp).reset_index(drop=True)\n",
    "    xs_raw = df[[\n",
    "        'ag_age',\n",
    "        'ag_sex',\n",
    "        'ag_eth',\n",
    "        'pt_nzdep',\n",
    "        'imp_hxdiab',\n",
    "        'pt_tc_hdl_ratio',\n",
    "        'pt_bps',\n",
    "        'pt_bpd',\n",
    "        'pt_smoke',\n",
    "        'imp_hxcvd',\n",
    "        'imp_hdl',\n",
    "        'imp_ldl',\n",
    "        'imp_tchol',\n",
    "        'marker',\n",
    "        'region',\n",
    "        'PH_BL_LLD_ANY',\n",
    "        'PH_BL_AHT_ANY',\n",
    "        'pt_familyhistory',\n",
    "        'ab_gen',\n",
    "        'eth_gen',\n",
    "        'is.female',\n",
    "        'log.age',\n",
    "        'log.age.gender',\n",
    "        'log.sbp',\n",
    "        'smoking',\n",
    "        'log.tchdl',\n",
    "        'diabetes',\n",
    "        'diabetes.sex']]\n",
    "    \n",
    "    xs_raw = xs_raw[[\n",
    "        'is.female', \n",
    "        'ag_age',\n",
    "        'pt_bps',\n",
    "        'smoking',\n",
    "        'pt_tc_hdl_ratio',\n",
    "        'diabetes'\n",
    "    ]]\n",
    "\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    scaler.fit(xs_raw)\n",
    "    xs = scaler.transform(xs_raw)\n",
    "    ys = df['dead'].apply(lambda x: 1 if x else 0)\n",
    "    ys = np.array(ys).astype(float)\n",
    "    if proportionValidation == 0:\n",
    "        return xs, [], ys, []\n",
    "    X_train, X_validate, y_train, y_validate = train_test_split(xs, ys, test_size=proportionValidation, random_state=randomSeed)\n",
    "    return X_train, X_validate, y_train, y_validate\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, _, y_test, __ = load_dataset('./../../data/processed/synthetic_view/synthetic_view_test.csv',\n",
    "                                    proportionValidation = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.95547078, -1.2099392 , -0.95424448, -0.86593193, -0.61149249,\n",
       "        -0.3018414 ],\n",
       "       [-1.04660448,  0.24162668,  0.25840041,  1.15482518, -1.58208848,\n",
       "        -0.3018414 ],\n",
       "       [ 0.95547078,  2.03234418,  0.99383009, -0.86593193, -0.06227458,\n",
       "        -0.3018414 ],\n",
       "       [-1.04660448, -1.38154001, -1.43596343,  1.15482518, -0.02036266,\n",
       "        -0.3018414 ],\n",
       "       [ 0.95547078,  0.74209164, -0.51778431,  1.15482518,  0.02995959,\n",
       "        -0.3018414 ]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "The NEAT implementation on which ExplaNEAT extends uses a single function call for evaluating fitness. Although this might be reworked for ExplaNEAT to be able to get consistency between the genome-evaluation and the backprop loss function, that can be reviewed later.\n",
    "\n",
    "This use `Binary Cross Entropy Loss` from `PyTorch`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def eval_genomes(genomes, config):\n",
    "#     loss = nn.BCELoss()\n",
    "#     loss = loss.to(device)\n",
    "#     for genome_id, genome in genomes:\n",
    "#         net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "#         preds = []\n",
    "#         for xi in X_validate:\n",
    "#             preds.append(1. if net.activate(xi)[0] > 0.5 else 0.)\n",
    "#         correct = 0\n",
    "#         for pred, truth in zip(preds, y_validate):\n",
    "#             if pred == truth:\n",
    "#                 correct += 1.\n",
    "        \n",
    "        \n",
    "#         genome.fitness = float(correct / len(preds))\n",
    "def eval_genomes(genomes, config):\n",
    "    loss = nn.BCELoss()\n",
    "    loss = loss.to(device)\n",
    "    for genome_id, genome in genomes:\n",
    "        net = neat.nn.FeedForwardNetwork.create(genome, config)\n",
    "        preds = []\n",
    "        for xi in X_validate:\n",
    "            preds.append(net.activate(xi))\n",
    "        genome.fitness = float(1./loss(torch.tensor(preds), torch.tensor(y_validate)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration\n",
    "\n",
    "We are going to create the base configuration according to an external configuration file. Per experiment, we will adjust this, later, but this defines the defaults across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config-synthview\"\n",
    "base_config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                     neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                     config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_config.pop_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to put a hard limit on how long this can go on for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "maxNGenerations = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a method to manage the instantiation of a population on the basis of a specific config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_population(config, xs, ys, saveLocation):\n",
    "\n",
    "    if not os.path.exists(saveLocation):\n",
    "        os.makedirs(saveLocation)\n",
    "        \n",
    "    config.save(os.path.join(saveLocation, 'config.conf'))\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = BackpropPopulation(config, \n",
    "                            xs, \n",
    "                            ys, \n",
    "                            criterion=nn.BCELoss())\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5, filename_prefix=str(saveLocation) + \"checkpoint-\" ))\n",
    "    bpReporter = backprop.BackpropReporter(True)\n",
    "    p.add_reporter(bpReporter)\n",
    "    p.add_reporter(ExperimentReporter(saveLocation))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment: Vary dataset size\n",
    "\n",
    "The first experiment is going to examine the difference from different dataset sizess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetSizes = [\n",
    "#         1000,\n",
    "#         2500,\n",
    "#         5000,\n",
    "        10000,\n",
    "        25000,\n",
    "#         50000,\n",
    "#         100000,\n",
    "#         250000,\n",
    "#         500000,\n",
    "#         1000000,\n",
    "#         1500000,\n",
    "#         2000000\n",
    "    ]\n",
    "# datasetSizes = [1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_config.pop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocationTemplate = './../../data/experiments/synthview/experiment-dataset-{}-{}/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetLocation = './../../data/processed/synthetic_view/'\n",
    "datasetFileTemplate = 'synthetic_view_test_{:07d}.csv'\n",
    "# os.path.join(output_filepath, 'synthetic_view_test_{:07d}.csv'.format(dsSize)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_validate, y_train, y_validate = load_dataset(os.path.join(datasetLocation, datasetFileTemplate.format(1000)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.96462528, -0.53797002, -0.9557672 ,  1.1560487 ,  0.48886043,\n",
       "        -0.31831052],\n",
       "       [-1.03667198,  1.76174535, -0.44512965, -0.86501546, -0.77320438,\n",
       "         3.14158642],\n",
       "       [-1.03667198,  0.08111723,  0.07925572, -0.86501546, -0.27909074,\n",
       "        -0.31831052],\n",
       "       [ 0.96462528, -0.03319542,  0.30570823,  1.1560487 , -0.8926339 ,\n",
       "        -0.31831052],\n",
       "       [ 0.96462528, -1.25741506, -1.46179745, -0.86501546, -1.27272262,\n",
       "        -0.31831052]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################################################\n",
      "################################################\n",
      "Starting dsSize 10000 iteration 0\n",
      "Started at 07/26/2019, 03:09:08\n",
      "################################################\n",
      "################################################\n",
      "\n",
      " ****** Running generation 0 ****** \n",
      "\n",
      "mean improvement: 0.0\n",
      "best improvement: tensor(0., grad_fn=<SubBackward0>)\n",
      "best loss: tensor(1.1979, grad_fn=<DivBackward0>)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/ipykernel_launcher.py:24: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "/opt/conda/lib/python3.6/site-packages/torch/nn/functional.py:2016: UserWarning: Using a target size (torch.Size([2000])) that is different to the input size (torch.Size([2000, 1])) is deprecated. Please ensure they have the same size.\n",
      "  \"Please ensure they have the same size.\".format(target.size(), input.size()))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Population's average fitness: 0.26199 stdev: 0.17336\n",
      "Best fitness: 0.87194 - size: (1, 6) - species 4 - id 32\n",
      "ending generation %s\n",
      "Average adjusted fitness: 0.272\n",
      "Mean genetic distance 3.704, standard deviation 1.682\n",
      "Population of 51 members in 4 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    0     5      0.1    0.025     0\n",
      "     2    0    15      0.8    0.395     0\n",
      "     3    0    20      0.6    0.153     0\n",
      "     4    0    11      0.9    0.515     0\n",
      "Total extinctions: 0\n",
      "Generation time: 1009.766 sec\n",
      "Saving checkpoint to ./../../data/experiments/synthview/experiment-dataset-10000-0/checkpoint-0\n",
      "\n",
      " ****** Running generation 1 ****** \n",
      "\n",
      "mean improvement: 0.0\n",
      "best improvement: tensor(0., grad_fn=<SubBackward0>)\n",
      "best loss: tensor(0.6104, grad_fn=<DivBackward0>)\n",
      "Population's average fitness: 0.53100 stdev: 0.34241\n",
      "Best fitness: 1.70247 - size: (2, 6) - species 2 - id 73\n",
      "\n",
      "\n",
      " SPECIES TOPOLOGY IMPROVEMENT\n",
      "\n",
      "\n",
      "{'genome': <neat.genome.DefaultGenome object at 0x7fe595d15c18>, 'fitness': 1.7024683952331543, 'firstDerivatives': [0.0, 0.8305292129516602], 'secondDerivatives': [0.0, 0.8305292129516602]}\n",
      "Key: 73\n",
      "Fitness: 1.7024683952331543\n",
      "Nodes:\n",
      "\t0 DefaultNodeGene(key=0, bias=-2.251121759414673, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "\t14 DefaultNodeGene(key=14, bias=-0.5348098278045654, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "Connections:\n",
      "\tDefaultConnectionGene(key=(-6, 0), weight=-0.3913094103336334, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-4, 0), weight=0.08813497424125671, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-3, 0), weight=0.217709481716156, enabled=False)\n",
      "\tDefaultConnectionGene(key=(-3, 14), weight=1.0, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-2, 0), weight=1.0935357809066772, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-1, 0), weight=0.17415738105773926, enabled=True)\n",
      "\tDefaultConnectionGene(key=(14, 0), weight=0.217709481716156, enabled=True)\n",
      "Nodes\n",
      "0    DefaultNodeGene(key=0, bias=-2.251121759414673, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "14    DefaultNodeGene(key=14, bias=-0.5348098278045654, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "Connections\n",
      "(-1, 0)    DefaultConnectionGene(key=(-1, 0), weight=0.17415738105773926, enabled=True)\n",
      "(-2, 0)    DefaultConnectionGene(key=(-2, 0), weight=1.0935357809066772, enabled=True)\n",
      "(-3, 0)    DefaultConnectionGene(key=(-3, 0), weight=0.217709481716156, enabled=False)\n",
      "(-4, 0)    DefaultConnectionGene(key=(-4, 0), weight=0.08813497424125671, enabled=True)\n",
      "(-6, 0)    DefaultConnectionGene(key=(-6, 0), weight=-0.3913094103336334, enabled=True)\n",
      "(-3, 14)    DefaultConnectionGene(key=(-3, 14), weight=1.0, enabled=True)\n",
      "(14, 0)    DefaultConnectionGene(key=(14, 0), weight=0.217709481716156, enabled=True)\n",
      "ending generation %s\n",
      "Average adjusted fitness: 0.221\n",
      "Mean genetic distance 3.136, standard deviation 1.425\n",
      "Population of 50 members in 4 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    1     3      0.2    0.010     0\n",
      "     2    1    24      1.7    0.501     0\n",
      "     3    1    14      0.6    0.178     0\n",
      "     4    1     9      0.6    0.194     1\n",
      "Total extinctions: 0\n",
      "Generation time: 1049.173 sec (1029.470 average)\n",
      "Saving checkpoint to ./../../data/experiments/synthview/experiment-dataset-10000-0/checkpoint-1\n",
      "\n",
      " ****** Running generation 2 ****** \n",
      "\n",
      "mean improvement: 0.0\n",
      "best improvement: tensor(0., grad_fn=<SubBackward0>)\n",
      "best loss: tensor(0.3987, grad_fn=<DivBackward0>)\n",
      "Population's average fitness: 1.03326 stdev: 0.58500\n",
      "Best fitness: 2.74535 - size: (2, 5) - species 2 - id 124\n",
      "\n",
      "\n",
      " SPECIES TOPOLOGY IMPROVEMENT\n",
      "\n",
      "\n",
      "{'genome': <neat.genome.DefaultGenome object at 0x7fe595d21358>, 'fitness': 2.7453458309173584, 'firstDerivatives': [0.0, 0.8305292129516602, 1.042877435684204], 'secondDerivatives': [0.0, 0.8305292129516602, 0.21234822273254395]}\n",
      "Key: 124\n",
      "Fitness: 2.7453458309173584\n",
      "Nodes:\n",
      "\t0 DefaultNodeGene(key=0, bias=-2.251121759414673, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "\t43 DefaultNodeGene(key=43, bias=1.0964465141296387, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "Connections:\n",
      "\tDefaultConnectionGene(key=(-6, 0), weight=-0.3913094103336334, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-4, 0), weight=0.08813497424125671, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-3, 0), weight=0.426864355802536, enabled=False)\n",
      "\tDefaultConnectionGene(key=(-2, 0), weight=1.442347764968872, enabled=False)\n",
      "\tDefaultConnectionGene(key=(-2, 43), weight=1.0, enabled=True)\n",
      "\tDefaultConnectionGene(key=(-1, 0), weight=0.17415738105773926, enabled=True)\n",
      "\tDefaultConnectionGene(key=(43, 0), weight=1.442347764968872, enabled=True)\n",
      "Nodes\n",
      "0    DefaultNodeGene(key=0, bias=-2.251121759414673, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "43    DefaultNodeGene(key=43, bias=1.0964465141296387, response=1.0, activation=sigmoid, aggregation=sum)\n",
      "Connections\n",
      "(-1, 0)    DefaultConnectionGene(key=(-1, 0), weight=0.17415738105773926, enabled=True)\n",
      "(-2, 0)    DefaultConnectionGene(key=(-2, 0), weight=1.442347764968872, enabled=False)\n",
      "(-3, 0)    DefaultConnectionGene(key=(-3, 0), weight=0.426864355802536, enabled=False)\n",
      "(-4, 0)    DefaultConnectionGene(key=(-4, 0), weight=0.08813497424125671, enabled=True)\n",
      "(-6, 0)    DefaultConnectionGene(key=(-6, 0), weight=-0.3913094103336334, enabled=True)\n",
      "(-2, 43)    DefaultConnectionGene(key=(-2, 43), weight=1.0, enabled=True)\n",
      "(43, 0)    DefaultConnectionGene(key=(43, 0), weight=1.442347764968872, enabled=True)\n",
      "ending generation %s\n",
      "Average adjusted fitness: 0.239\n",
      "Mean genetic distance 3.071, standard deviation 1.536\n",
      "Population of 50 members in 4 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    2     2      0.2    0.014     0\n",
      "     2    2    25      2.7    0.517     0\n",
      "     3    2    12      1.2    0.188     0\n",
      "     4    2    11      1.2    0.238     0\n",
      "Total extinctions: 0\n",
      "Generation time: 1071.925 sec (1043.621 average)\n",
      "Saving checkpoint to ./../../data/experiments/synthview/experiment-dataset-10000-0/checkpoint-2\n",
      "\n",
      " ****** Running generation 3 ****** \n",
      "\n",
      "mean improvement: 0.0\n",
      "best improvement: tensor(0., grad_fn=<SubBackward0>)\n",
      "best loss: tensor(0.3241, grad_fn=<DivBackward0>)\n",
      "Population's average fitness: 1.48274 stdev: 0.71490\n",
      "Best fitness: 3.33211 - size: (2, 5) - species 2 - id 172\n",
      "ending generation %s\n",
      "Average adjusted fitness: 0.295\n",
      "Mean genetic distance 2.897, standard deviation 1.389\n",
      "Population of 50 members in 4 species:\n",
      "   ID   age  size  fitness  adj fit  stag\n",
      "  ====  ===  ====  =======  =======  ====\n",
      "     1    3     2      0.2    0.011     1\n",
      "     2    3    24      3.3    0.546     0\n",
      "     3    3    14      3.2    0.368     0\n",
      "     4    3    10      1.4    0.257     0\n",
      "Total extinctions: 0\n",
      "Generation time: 1049.425 sec (1045.072 average)\n",
      "Saving checkpoint to ./../../data/experiments/synthview/experiment-dataset-10000-0/checkpoint-3\n",
      "\n",
      " ****** Running generation 4 ****** \n",
      "\n"
     ]
    }
   ],
   "source": [
    "for dsSize in datasetSizes:\n",
    "    for iteration_no in range(5):\n",
    "        \n",
    "        X_train, X_validate, y_train, y_validate = load_dataset(os.path.join(datasetLocation, datasetFileTemplate.format(dsSize)))\n",
    "        X_train = torch.tensor(X_train)\n",
    "        X_validate = torch.tensor(X_validate)\n",
    "        y_train = torch.tensor(y_train).float()\n",
    "        y_validate = torch.tensor(y_validate).float()\n",
    "        \n",
    "        \n",
    "        my_random_seed += 1\n",
    "        random.seed(my_random_seed)\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "        print(\"Starting dsSize {} iteration {}\".format(dsSize, iteration_no))\n",
    "        print(\"Started at {}\".format(start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "        \n",
    "        \n",
    "        config = deepcopy(base_config)\n",
    "        \n",
    "        saveLocation = saveLocationTemplate.format(dsSize, iteration_no)\n",
    "        \n",
    "        p = instantiate_population(config, X_train, y_train, saveLocation)\n",
    "        # Run for up to nGenerations generations.\n",
    "        winner = p.run(eval_genomes, maxNGenerations, nEpochs = 10)\n",
    "        \n",
    "        g = p.best_genome\n",
    "\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        p.reporters.reporters[2].save_checkpoint(p.config, p.population, p.species, str(p.generation) + \"-final\")  \n",
    "        \n",
    "        winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "\n",
    "        results = []\n",
    "        for xi, xo in zip(X_test, y_test):\n",
    "            output = winner_net.activate(xi)\n",
    "            results.append([xi, xo, output])\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(os.path.join(saveLocation, 'results.csv'))\n",
    "        \n",
    "        ancestry = p.reporters.reporters[3].trace_ancestry_of_species(g.key, p.reproduction.ancestors) \n",
    "\n",
    "        ancestors = {\n",
    "            k: v['genome'] for k, v in p.reporters.reporters[3].ancestry.items()\n",
    "        }\n",
    "        \n",
    "        \n",
    "        \n",
    "        ## Save all of these to disc\n",
    "        filename = 'fullStatus.xplnt'\n",
    "        print(\"Saving checkpoint to {0}\".format(filename))\n",
    "\n",
    "        with gzip.open(os.path.join(saveLocation, filename), 'w', compresslevel=5) as f:\n",
    "            data = (p, g, ancestry, ancestors, random.getstate())\n",
    "            pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "#         visualize.create_ancestry_video(p.config, \n",
    "#                                         g, \n",
    "#                                         ancestry, \n",
    "#                                         ancestors, \n",
    "#                                         p.reporters.reporters[1], \n",
    "#                                         pathname=saveLocation)\n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "        print(\"Have finished dsSize {} iteration {}\".format(dsSize, iteration_no))\n",
    "        print(\"Started at {}\".format(start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        print(\"The time is {}\".format(end_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        print(\"################################################\")\n",
    "        print(\"################################################\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
