{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explaneat.data.uci import UCI_WRANGLER\n",
    "from explaneat.experimenter.experiment import GenericExperiment\n",
    "from explaneat.evaluators.evaluators import binary_cross_entropy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import neat\n",
    "\n",
    "from explaneat.core.backprop import NeatNet\n",
    "from explaneat.core import backprop\n",
    "from explaneat.core.backproppop import BackpropPopulation\n",
    "from explaneat.visualization import visualize\n",
    "from explaneat.core.experiment import ExperimentReporter\n",
    "from explaneat.core.utility import one_hot_encode\n",
    "\n",
    "from explaneat.experimenter.results import Result, ResultsDatabase\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 13:57:26,968 - experimenter - INFO - Validating configuration schema\n",
      "2022-06-12 13:57:26,968 - experimenter - INFO - Validating configuration schema\n",
      "2022-06-12 13:57:26,970 - experimenter - INFO - Schema validation passed\n",
      "2022-06-12 13:57:26,970 - experimenter - INFO - Schema validation passed\n",
      "2022-06-12 13:57:26,971 - experimenter - INFO - Starting to create folder structures\n",
      "2022-06-12 13:57:26,971 - experimenter - INFO - Starting to create folder structures\n",
      "2022-06-12 13:57:26,971 - experimenter - INFO - Experiment folder name is test_experiment_220612T135726_e80c82d2\n",
      "2022-06-12 13:57:26,971 - experimenter - INFO - Experiment folder name is test_experiment_220612T135726_e80c82d2\n",
      "2022-06-12 13:57:26,972 - experimenter - INFO - Experiment root path is ./../../data/experiments/tests/test_experiment_220612T135726_e80c82d2\n",
      "2022-06-12 13:57:26,972 - experimenter - INFO - Experiment root path is ./../../data/experiments/tests/test_experiment_220612T135726_e80c82d2\n",
      "2022-06-12 13:57:26,972 - experimenter - INFO - Creating the root path\n",
      "2022-06-12 13:57:26,972 - experimenter - INFO - Creating the root path\n",
      "2022-06-12 13:57:26,973 - experimenter - INFO - Root path created\n",
      "2022-06-12 13:57:26,973 - experimenter - INFO - Root path created\n",
      "2022-06-12 13:57:26,973 - experimenter - INFO - Creating results\n",
      "2022-06-12 13:57:26,973 - experimenter - INFO - Creating results\n",
      "2022-06-12 13:57:26,974 - experimenter - INFO - Creating results/interim\n",
      "2022-06-12 13:57:26,974 - experimenter - INFO - Creating results/interim\n",
      "2022-06-12 13:57:26,975 - experimenter - INFO - Creating results/final\n",
      "2022-06-12 13:57:26,975 - experimenter - INFO - Creating results/final\n",
      "2022-06-12 13:57:26,975 - experimenter - INFO - Creating configurations\n",
      "2022-06-12 13:57:26,975 - experimenter - INFO - Creating configurations\n",
      "2022-06-12 13:57:26,976 - experimenter - INFO - Creating logs\n",
      "2022-06-12 13:57:26,976 - experimenter - INFO - Creating logs\n",
      "2022-06-12 13:57:26,976 - experimenter - INFO - All folders created\n",
      "2022-06-12 13:57:26,976 - experimenter - INFO - All folders created\n",
      "2022-06-12 13:57:26,977 - experimenter - INFO - Creating file logging\n",
      "2022-06-12 13:57:26,977 - experimenter - INFO - Creating file logging\n",
      "2022-06-12 13:57:26,977 - experimenter - INFO - Finished creating file logging\n",
      "2022-06-12 13:57:26,977 - experimenter - INFO - Finished creating file logging\n",
      "2022-06-12 13:57:26,978 - experimenter - INFO - Saving experiment configuration\n",
      "2022-06-12 13:57:26,978 - experimenter - INFO - Saving experiment configuration\n",
      "2022-06-12 13:57:26,979 - experimenter - INFO - Saving other config files\n",
      "2022-06-12 13:57:26,979 - experimenter - INFO - Saving other config files\n",
      "2022-06-12 13:57:26,979 - experimenter - INFO - Experiment init for project e80c82d2 has completed\n",
      "2022-06-12 13:57:26,979 - experimenter - INFO - Experiment init for project e80c82d2 has completed\n"
     ]
    }
   ],
   "source": [
    "experiment_config_file = './experiment_config.json'\n",
    "experiment = GenericExperiment(experiment_config_file, confirm_path_creation=False)\n",
    "results = ResultsDatabase(experiment.config['results']['database'])\n",
    "logger = experiment.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 13:53:15,955 - experimenter - INFO - Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n",
    "logger.info(\"Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC Experiment\n",
    "\n",
    "This experiment (a) test the experimental environment, but is also to evaluate the efficacy of the ExplaNEAT algorithm. Speed is a critical factor, as well as stability of results on population size. Total run time will also be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a random seed and a total stopping point in the number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-12 13:53:15,968 - experimenter - INFO - random.seed set to 42\n"
     ]
    }
   ],
   "source": [
    "random.seed(experiment.config[\"random_seed\"])\n",
    "logger.info(\"random.seed set to {}\".format(experiment.config[\"random_seed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to work with the Iris dataset, which will be loaded from `sklearn`. We want to characterise the efficacy of the algorithm with regards to a mostly untransformed dataset, so we will only normalise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m data_wrangler \u001b[38;5;241m=\u001b[39m \u001b[43mUCI_WRANGLER\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_location\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m      2\u001b[0m \u001b[43m        \u001b[49m\u001b[43mexperiment\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mraw_data_meta\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dev-mtm/phd-neat-experiments/explaneat/data/uci.py:39\u001b[0m, in \u001b[0;36mUCI_WRANGLER.__init__\u001b[0;34m(self, data_file, meta_file, config, logger)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlogger \u001b[38;5;241m=\u001b[39m logger\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(data_file):\n\u001b[0;32m---> 39\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     41\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata_file \u001b[38;5;241m=\u001b[39m data_file\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data_wrangler = UCI_WRANGLER(experiment.config['data']['raw_location'],\n",
    "        experiment.config['data']['raw_data_meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler.create_train_test_split(experiment.config[\"train_test_ratio\"],\n",
    "            experiment.config[\"random_seed\"])\n",
    "data_wrangler.send_train_test_to_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "The NEAT implementation on which ExplaNEAT extends uses a single function call for evaluating fitness. Although this might be reworked for ExplaNEAT to be able to get consistency between the genome-evaluation and the backprop loss function, that can be reviewed later.\n",
    "\n",
    "This use `CrossEntropyLoss` from `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration\n",
    "\n",
    "We are going to create the base configuration according to an external configuration file. Per experiment, we will adjust this, later, but this defines the defaults across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config-bchard\"\n",
    "base_config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                     neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                     config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.register_config_file(\"./config-bchard\", \"neat_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to put a hard limit on how long this can go on for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a method to manage the instantiation of a population on the basis of a specific config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_population(config, xs, ys, saveLocation):\n",
    "\n",
    "    if not os.path.exists(saveLocation):\n",
    "        os.makedirs(saveLocation)\n",
    "        \n",
    "    config.save(os.path.join(saveLocation, 'config.conf'))\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = BackpropPopulation(config, \n",
    "                            xs, \n",
    "                            ys, \n",
    "                            criterion=nn.BCELoss())\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5, filename_prefix=str(saveLocation) + \"checkpoint-\" ))\n",
    "    bpReporter = backprop.BackpropReporter(True)\n",
    "    p.add_reporter(bpReporter)\n",
    "    p.add_reporter(ExperimentReporter(saveLocation))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Vary population size\n",
    "\n",
    "The first experiment is going to examine the difference in run time different population sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_points = [10, 25, 50, 100, 150]\n",
    "# epoch_points = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config.pop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocationTemplate = './../../data/experiments/bchard/experiment-test-{}-{}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_wrangler.X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler.ys.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_random_seed = experiment.config[\"random_seed\"]\n",
    "for epochs in epoch_points:\n",
    "    for iteration_no in range(5):\n",
    "        my_random_seed += 1\n",
    "        random.seed(my_random_seed)\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"Starting epochs {} iteration {}\".format(epochs, iteration_no))\n",
    "        logger.info(\"Started at {}\".format(start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"################################################\")\n",
    "        \n",
    "        \n",
    "        config = deepcopy(base_config)\n",
    "#         config.pop_size = pop_size\n",
    "        \n",
    "        saveLocation = saveLocationTemplate.format(epochs, iteration_no)\n",
    "        \n",
    "        p = instantiate_population(config, data_wrangler.X_train, data_wrangler.y_train, saveLocation)\n",
    "        # Run for up to nGenerations generations.\n",
    "        winner = p.run(binary_cross_entropy, experiment.config[\"max_n_generations\"], nEpochs = epochs)\n",
    "        \n",
    "        g = p.best_genome\n",
    "\n",
    "        g_result = Result(\n",
    "            g,\n",
    "            \"best_genome\",\n",
    "            experiment.config['experiment']['name'],\n",
    "            experiment.config['data']['raw_location'],\n",
    "            experiment.experiment_sha,\n",
    "            iteration_no,\n",
    "            {\n",
    "                \"iteration\":iteration_no,\n",
    "                \"n_epochs\":epoch_points\n",
    "            }\n",
    "        )\n",
    "        results.add_result(g_result)\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        p.reporters.reporters[2].save_checkpoint(p.config, p.population, p.species, str(p.generation) + \"-final\")  \n",
    "        \n",
    "        winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "\n",
    "        results = []\n",
    "        for xi, xo in zip(data_wrangler.X_test, data_wrangler.y_test):\n",
    "            output = winner_net.activate(xi)\n",
    "            results.append([xi, xo, output])\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(os.path.join(saveLocation, 'results.csv'))\n",
    "\n",
    "        g_result = Result(\n",
    "            df,\n",
    "            \"results\",\n",
    "            experiment.config['experiment']['name'],\n",
    "            experiment.config['data']['raw_location'],\n",
    "            experiment.experiment_sha,\n",
    "            iteration_no,\n",
    "            {\n",
    "                \"iteration\":iteration_no,\n",
    "                \"n_epochs\":epoch_points\n",
    "            }\n",
    "        )\n",
    "        results.add_result(g_result)\n",
    "\n",
    "        ancestry = p.reporters.reporters[3].trace_ancestry_of_species(g.key, p.reproduction.ancestors) \n",
    "\n",
    "        ancestors = {\n",
    "            k: v['genome'] for k, v in p.reporters.reporters[3].ancestry.items()\n",
    "        }\n",
    "\n",
    "        ## Save all of these to disc\n",
    "        filename = 'fullStatus.xplnt'\n",
    "        logger.info(\"Saving checkpoint to {0}\".format(filename))\n",
    "\n",
    "#         with gzip.open(os.path.join(saveLocation, filename), 'w', compresslevel=5) as f:\n",
    "#             data = (p, g, ancestry, ancestors, random.getstate())\n",
    "#             pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "#         with gzip.open(os.path.join(saveLocation, 'train_test_data.pkl'), 'w', compresslevel=5) as f:\n",
    "#             train_Test = (data_wrangler.X_train, data_wrangler.X_test, data_wrangler.y_train, data_wrangler.y_test)\n",
    "#             pickle.dump(train_Test, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}