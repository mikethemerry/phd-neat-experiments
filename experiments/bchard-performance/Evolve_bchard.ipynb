{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from explaneat.data.uci import UCI_WRANGLER\n",
    "from explaneat.experimenter.experiment import GenericExperiment\n",
    "from explaneat.evaluators.evaluators import binary_cross_entropy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "import logging\n",
    "import random\n",
    "\n",
    "import os\n",
    "\n",
    "import neat\n",
    "\n",
    "from explaneat.core.backprop import NeatNet\n",
    "from explaneat.core import backprop\n",
    "from explaneat.core.backproppop import BackpropPopulation\n",
    "from explaneat.visualization import visualize\n",
    "from explaneat.core.experiment import ExperimentReporter\n",
    "from explaneat.core.utility import one_hot_encode\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import gzip\n",
    "try:\n",
    "    import cPickle as pickle  # pylint: disable=import-error\n",
    "except ImportError:\n",
    "    import pickle  # pylint: disable=import-error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-06-01 11:12:45,221 - experimenter - INFO - Validating configuration schema\n",
      "2022-06-01 11:12:45,226 - experimenter - INFO - Schema validation passed\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "SHA could not be resolved, git returned: b''",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[0;32mIn [2]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m experiment_config_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./experiment_config.json\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m experiment \u001b[38;5;241m=\u001b[39m \u001b[43mGenericExperiment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mexperiment_config_file\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfirm_path_creation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m logger \u001b[38;5;241m=\u001b[39m experiment\u001b[38;5;241m.\u001b[39mlogger\n",
      "File \u001b[0;32m~/app/explaneat/experimenter/experiment.py:69\u001b[0m, in \u001b[0;36mGenericExperiment.__init__\u001b[0;34m(self, config, confirm_path_creation, experiment_sha, logging_level)\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_requested_config \u001b[38;5;241m=\u001b[39m config\n\u001b[1;32m     67\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mload_config()\n\u001b[0;32m---> 69\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_device_and_repo_to_config\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig_string \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mdumps(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig)\n\u001b[1;32m     73\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfirm_path_creation \u001b[38;5;241m=\u001b[39m confirm_path_creation\n",
      "File \u001b[0;32m~/app/explaneat/experimenter/experiment.py:240\u001b[0m, in \u001b[0;36mGenericExperiment.add_device_and_repo_to_config\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    235\u001b[0m     have_good_repo \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrepository\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    239\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbranch\u001b[39m\u001b[38;5;124m\"\u001b[39m: repo\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mreference\u001b[38;5;241m.\u001b[39mname,\n\u001b[0;32m--> 240\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcommit\u001b[49m\u001b[38;5;241m.\u001b[39mhexsha,\n\u001b[1;32m    241\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchanges\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m    242\u001b[0m         diff\u001b[38;5;241m.\u001b[39ma_path \u001b[38;5;28;01mfor\u001b[39;00m diff \u001b[38;5;129;01min\u001b[39;00m repo\u001b[38;5;241m.\u001b[39mhead\u001b[38;5;241m.\u001b[39mcommit\u001b[38;5;241m.\u001b[39mdiff(\u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[1;32m    243\u001b[0m     ]\n\u001b[1;32m    244\u001b[0m }\n\u001b[1;32m    246\u001b[0m svmem \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mvirtual_memory()\n\u001b[1;32m    247\u001b[0m swap \u001b[38;5;241m=\u001b[39m psutil\u001b[38;5;241m.\u001b[39mswap_memory()\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/refs/symbolic.py:217\u001b[0m, in \u001b[0;36mSymbolicReference._get_commit\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    212\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_commit\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCommit\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    213\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    214\u001b[0m \u001b[38;5;124;03m    :return:\u001b[39;00m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;124;03m        Commit object we point to, works for detached and non-detached\u001b[39;00m\n\u001b[1;32m    216\u001b[0m \u001b[38;5;124;03m        SymbolicReferences. The symbolic reference will be dereferenced recursively.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m     obj \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_object\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m obj\u001b[38;5;241m.\u001b[39mtype \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtag\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    219\u001b[0m         obj \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mobject\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/refs/symbolic.py:210\u001b[0m, in \u001b[0;36mSymbolicReference._get_object\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;124;03m:return:\u001b[39;00m\n\u001b[1;32m    206\u001b[0m \u001b[38;5;124;03m    The object our ref currently refers to. Refs can be cached, they will\u001b[39;00m\n\u001b[1;32m    207\u001b[0m \u001b[38;5;124;03m    always point to the actual object as it gets re-created on each query\"\"\"\u001b[39;00m\n\u001b[1;32m    208\u001b[0m \u001b[38;5;66;03m# have to be dynamic here as we may be a tag which can point to anything\u001b[39;00m\n\u001b[1;32m    209\u001b[0m \u001b[38;5;66;03m# Our path will be resolved to the hexsha which will be used accordingly\u001b[39;00m\n\u001b[0;32m--> 210\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mObject\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew_from_sha\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhex_to_bin\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdereference_recursive\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/objects/base.py:85\u001b[0m, in \u001b[0;36mObject.new_from_sha\u001b[0;34m(cls, repo, sha1)\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m get_object_type_by_name(\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcommit\u001b[39m\u001b[38;5;124m'\u001b[39m)(repo, sha1)\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# END handle special case\u001b[39;00m\n\u001b[0;32m---> 85\u001b[0m oinfo \u001b[38;5;241m=\u001b[39m \u001b[43mrepo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43modb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minfo\u001b[49m\u001b[43m(\u001b[49m\u001b[43msha1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     86\u001b[0m inst \u001b[38;5;241m=\u001b[39m get_object_type_by_name(oinfo\u001b[38;5;241m.\u001b[39mtype)(repo, oinfo\u001b[38;5;241m.\u001b[39mbinsha)\n\u001b[1;32m     87\u001b[0m inst\u001b[38;5;241m.\u001b[39msize \u001b[38;5;241m=\u001b[39m oinfo\u001b[38;5;241m.\u001b[39msize\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/db.py:43\u001b[0m, in \u001b[0;36mGitCmdObjectDB.info\u001b[0;34m(self, binsha)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21minfo\u001b[39m(\u001b[38;5;28mself\u001b[39m, binsha: \u001b[38;5;28mbytes\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m OInfo:\n\u001b[0;32m---> 43\u001b[0m     hexsha, typename, size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_git\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_object_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbin_to_hex\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbinsha\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m OInfo(hex_to_bin(hexsha), typename, size)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/cmd.py:1253\u001b[0m, in \u001b[0;36mGit.get_object_header\u001b[0;34m(self, ref)\u001b[0m\n\u001b[1;32m   1245\u001b[0m \u001b[38;5;124;03m\"\"\" Use this method to quickly examine the type and size of the object behind\u001b[39;00m\n\u001b[1;32m   1246\u001b[0m \u001b[38;5;124;03mthe given ref.\u001b[39;00m\n\u001b[1;32m   1247\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1250\u001b[0m \n\u001b[1;32m   1251\u001b[0m \u001b[38;5;124;03m:return: (hexsha, type_string, size_as_int)\"\"\"\u001b[39;00m\n\u001b[1;32m   1252\u001b[0m cmd \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_persistent_cmd(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_file_header\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcat_file\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch_check\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m-> 1253\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_object_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mref\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/cmd.py:1240\u001b[0m, in \u001b[0;36mGit.__get_object_header\u001b[0;34m(self, cmd, ref)\u001b[0m\n\u001b[1;32m   1238\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mwrite(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_prepare_ref(ref))\n\u001b[1;32m   1239\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mstdin\u001b[38;5;241m.\u001b[39mflush()\n\u001b[0;32m-> 1240\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parse_object_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstdout\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1241\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1242\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcmd stdin was empty\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/opt/conda/lib/python3.8/site-packages/git/cmd.py:1198\u001b[0m, in \u001b[0;36mGit._parse_object_header\u001b[0;34m(self, header_line)\u001b[0m\n\u001b[1;32m   1196\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(tokens) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m3\u001b[39m:\n\u001b[1;32m   1197\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m tokens:\n\u001b[0;32m-> 1198\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHA could not be resolved, git returned: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (header_line\u001b[38;5;241m.\u001b[39mstrip()))\n\u001b[1;32m   1199\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1200\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSHA \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m could not be resolved, git returned: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m (tokens[\u001b[38;5;241m0\u001b[39m], header_line\u001b[38;5;241m.\u001b[39mstrip()))\n",
      "\u001b[0;31mValueError\u001b[0m: SHA could not be resolved, git returned: b''"
     ]
    }
   ],
   "source": [
    "experiment_config_file = './experiment_config.json'\n",
    "experiment = GenericExperiment(experiment_config_file, confirm_path_creation=False)\n",
    "logger = experiment.logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "USE_CUDA = torch.cuda.is_available()\n",
    "USE_CUDA = False\n",
    "device = torch.device(\"cuda:1\" if USE_CUDA else \"cpu\")\n",
    "logger.info(\"Using device: {}\".format(device))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BC Experiment\n",
    "\n",
    "This experiment (a) test the experimental environment, but is also to evaluate the efficacy of the ExplaNEAT algorithm. Speed is a critical factor, as well as stability of results on population size. Total run time will also be measured"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we need to set a random seed and a total stopping point in the number of generations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random.seed(experiment.config[\"random_seed\"])\n",
    "logger.info(\"random.seed set to {}\".format(experiment.config[\"random_seed\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset\n",
    "\n",
    "We are going to work with the Iris dataset, which will be loaded from `sklearn`. We want to characterise the efficacy of the algorithm with regards to a mostly untransformed dataset, so we will only normalise the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler = UCI_WRANGLER(experiment.config['data']['raw_location'],\n",
    "        experiment.config['data']['raw_data_meta'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler.create_train_test_split(experiment.config[\"train_test_ratio\"],\n",
    "            experiment.config[\"random_seed\"])\n",
    "data_wrangler.send_train_test_to_device(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance metric\n",
    "\n",
    "The NEAT implementation on which ExplaNEAT extends uses a single function call for evaluating fitness. Although this might be reworked for ExplaNEAT to be able to get consistency between the genome-evaluation and the backprop loss function, that can be reviewed later.\n",
    "\n",
    "This use `CrossEntropyLoss` from `PyTorch`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Base configuration\n",
    "\n",
    "We are going to create the base configuration according to an external configuration file. Per experiment, we will adjust this, later, but this defines the defaults across all runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_path = \"./config-bchard\"\n",
    "base_config = neat.Config(neat.DefaultGenome, neat.DefaultReproduction,\n",
    "                     neat.DefaultSpeciesSet, neat.DefaultStagnation,\n",
    "                     config_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "experiment.register_config_file(\"./config-bchard\", \"neat_config\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also want to put a hard limit on how long this can go on for."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will create a method to manage the instantiation of a population on the basis of a specific config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def instantiate_population(config, xs, ys, saveLocation):\n",
    "\n",
    "    if not os.path.exists(saveLocation):\n",
    "        os.makedirs(saveLocation)\n",
    "        \n",
    "    config.save(os.path.join(saveLocation, 'config.conf'))\n",
    "\n",
    "    # Create the population, which is the top-level object for a NEAT run.\n",
    "    p = BackpropPopulation(config, \n",
    "                            xs, \n",
    "                            ys, \n",
    "                            criterion=nn.BCELoss())\n",
    "\n",
    "    # Add a stdout reporter to show progress in the terminal.\n",
    "    p.add_reporter(neat.StdOutReporter(True))\n",
    "    stats = neat.StatisticsReporter()\n",
    "    p.add_reporter(stats)\n",
    "    p.add_reporter(neat.Checkpointer(5, filename_prefix=str(saveLocation) + \"checkpoint-\" ))\n",
    "    bpReporter = backprop.BackpropReporter(True)\n",
    "    p.add_reporter(bpReporter)\n",
    "    p.add_reporter(ExperimentReporter(saveLocation))\n",
    "    \n",
    "    return p"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiment 1: Vary population size\n",
    "\n",
    "The first experiment is going to examine the difference in run time different population sizes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epoch_points = [10, 25, 50, 100, 150]\n",
    "# epoch_points = [10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_config.pop_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "saveLocationTemplate = './../../data/experiments/bchard/experiment-test-{}-{}/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start the experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(data_wrangler.X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_wrangler.ys.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_random_seed = experiment.config[\"random_seed\"]\n",
    "for epochs in epoch_points:\n",
    "    for iteration_no in range(5):\n",
    "        my_random_seed += 1\n",
    "        random.seed(my_random_seed)\n",
    "        start_time = datetime.now()\n",
    "        \n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"Starting epochs {} iteration {}\".format(epochs, iteration_no))\n",
    "        logger.info(\"Started at {}\".format(start_time.strftime(\"%m/%d/%Y, %H:%M:%S\")))\n",
    "        logger.info(\"################################################\")\n",
    "        logger.info(\"################################################\")\n",
    "        \n",
    "        \n",
    "        config = deepcopy(base_config)\n",
    "#         config.pop_size = pop_size\n",
    "        \n",
    "        saveLocation = saveLocationTemplate.format(epochs, iteration_no)\n",
    "        \n",
    "        p = instantiate_population(config, data_wrangler.X_train, data_wrangler.y_train, saveLocation)\n",
    "        # Run for up to nGenerations generations.\n",
    "        winner = p.run(binary_cross_entropy, experiment.config[\"max_n_generations\"], nEpochs = epochs)\n",
    "        \n",
    "        g = p.best_genome\n",
    "\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        \n",
    "        p.reporters.reporters[2].save_checkpoint(p.config, p.population, p.species, str(p.generation) + \"-final\")  \n",
    "        \n",
    "        winner_net = neat.nn.FeedForwardNetwork.create(winner, config)\n",
    "\n",
    "        results = []\n",
    "        for xi, xo in zip(data_wrangler.X_test, data_wrangler.y_test):\n",
    "            output = winner_net.activate(xi)\n",
    "            results.append([xi, xo, output])\n",
    "\n",
    "        df = pd.DataFrame(results)\n",
    "        df.to_csv(os.path.join(saveLocation, 'results.csv'))\n",
    "\n",
    "        ancestry = p.reporters.reporters[3].trace_ancestry_of_species(g.key, p.reproduction.ancestors) \n",
    "\n",
    "        ancestors = {\n",
    "            k: v['genome'] for k, v in p.reporters.reporters[3].ancestry.items()\n",
    "        }\n",
    "\n",
    "        ## Save all of these to disc\n",
    "        filename = 'fullStatus.xplnt'\n",
    "        logger.info(\"Saving checkpoint to {0}\".format(filename))\n",
    "\n",
    "#         with gzip.open(os.path.join(saveLocation, filename), 'w', compresslevel=5) as f:\n",
    "#             data = (p, g, ancestry, ancestors, random.getstate())\n",
    "#             pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "            \n",
    "            \n",
    "#         with gzip.open(os.path.join(saveLocation, 'train_test_data.pkl'), 'w', compresslevel=5) as f:\n",
    "#             train_Test = (data_wrangler.X_train, data_wrangler.X_test, data_wrangler.y_train, data_wrangler.y_test)\n",
    "#             pickle.dump(train_Test, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
